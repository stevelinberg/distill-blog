[
  {
    "path": "academics/ps753/",
    "title": "PS753",
    "description": "Political and Social Networks, Spring 2022.",
    "author": [],
    "date": "2022-03-20",
    "categories": [
      "social networks"
    ],
    "contents": "\n\n > Academics > PS 753\n\nWork and assignments from this class will be added here as we go. (This part has to be done by hand in Distill, unfortunately.)\nMy ongoing glossary page; feel free to comment, or make additions or corrections via comments or github.\nSome useful R tidbits from the course material and related sources.\nHomework 3 for the course: a network analysis of the Florentine Wedding data.\nHomework 4 for the course: a centrality analysis of the Florentine Wedding data.\nHomework 5 for the course: brokerage and betweenness analysis of the Florentine Wedding data.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-20T19:47:01-04:00",
    "input_file": "ps753.knit.md"
  },
  {
    "path": "academics/ms797/",
    "title": "MS797",
    "description": "Machine Learning for Social Sciences, Spring 2022.",
    "author": [],
    "date": "2022-02-20",
    "categories": [
      "machine learning"
    ],
    "contents": "\n\n > Academics > MS797\n\nWork and assignments from this class will be added here as we go. (This part has to be done by hand in Distill, unfortunately.)\nMy ongoing glossary page; feel free to comment, or make additions or corrections via comments or github.\nSome useful R tidbits from, or inspired by, the text.\nBigger concepts about things too big to be glossary entries, like discussions of types of regression or classification.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-20T21:00:17-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ms797-bigger-concepts/",
    "title": "MS797 Bigger Concepts",
    "description": "Bigger concepts for MS797 coursework",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-19",
    "categories": [
      "machine learning"
    ],
    "contents": "\n\nContents\nClassification\nLogistic Regression\nWhen to use logistic regression\nHow to do logistic regression\n\nLinear Discriminant Analysis (LDA)\nResampling methods\nCross-validation: validation set approach\nCross-validation: leave-one-out (LOOCV)\nCross-validation: k-fold\n\n\n\n\n > Academics > MS797 > MS797 Bigger Concepts\n\nThis page is for concepts that don’t fit into the glossary.\nThis document is in process.\nClassification\nThe general umbrella categories for classification—where the outcome variable to be predicted is categorical, rather than continuous, like eye color rather than weight—are “discriminative” and “generative”. Discriminative models take sets of data and separate them into classes, like finding the decision boundary between two or more classes, but can’t predict the values of new data. Generative models (used in unsupervised learning) can generate new data, but are more susceptible to outliers (misclassified observations) than discriminative models, require more data to create, and are more computationally expensive.\nA generative model—somehow—tries to guess how the data was generated, so it can take a new bit of unseen data, assess the probability that each class would have produced it, and pick the class with the highest probability.\nA discriminative model can only look at existing data and create a decision boundary separating the existing data into classes.\nFigure from https://stanford.edu/%7Eshervine/teaching/cs-229/cheatsheet-supervised-learningNote that there is no decision boundary in the generative model, just levels of probability into which new data could be classified with varying levels of confidence.\nLogistic regression is discriminative. Linear discriminant analysis, quadratic discriminant analysis, and naive Bayes are generative, K-nearest neighbors appears to be neither.\nLogistic Regression\nLogistic regression’s output is a probability that a response variable belongs to a particular category.\nThe general form of a logistic model is:\n\\[p(X) = \\text{PR}(Y = 1|X)\\] The right side of the equation reads “The probability that Y equals 1, given X” - or the chance that Y is a particular category for a specific value of X.\nA linear regression model would use this form:\n\\[p(X) = \\beta_0 + \\beta_1X\\]\nAnd the magic formula for this is the logistic function, in which we raise \\(e\\) to this power.\n\\[p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\]\n(Why \\(e^{\\beta_0 + \\beta_1X}\\) instead of, say, \\(2^{\\beta_0 + \\beta_1X}\\) or \\(\\pi^{\\beta_0 + \\beta_1X}\\) or \\(123456^{\\beta_0 + \\beta_1X}\\)?)\nDividing both sides by \\(1 - p(X)\\) (somehow) yields:\n\\[\\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}\\]\nwhich is also the odds that Y is 1 given X. If the probability of something is 20%, or 1/5, then the odds are 1/4:\n\\[\\frac{1}{5} \\div \\left[ 1 - \\frac{1}{5} \\right] = \\frac{1}{5} \\div \\frac{4}{5} = \\frac{1}{5} \\times \\frac{5}{4} = 1/4 = 0.25\\]\nWhen to use logistic regression\nWhen the response variable is categorical, and has two possible classes, like “Yes” or “No”, “Up” or “Down”, etc. However, see Linear Discriminant Analysis below for examples of when not to use it.\nIn some cases, a categorical response variable could be analyzed with a linear regression by treating the categorical variable as numeric, like treating “not infected” as 0 and “infected” as 1, and using the rounded value as the predictor; however, beyond 2 possible responses it’s unlikely that a continuous scale could be established between the possible values, and it gets messy.\nHow to do logistic regression\nUse glm() with the argument family = binomial to indicate logistic regression.\nUsing the Boston data set from the ISLR2 package, the following will model the probability of a tract having a crim rate above the median, based on zn, using logistic regression:\n\n\nShow code\n\ndata(Boston)\nboston <- Boston %>%\n  # Create the crim_above_median response variable\n  mutate(crim_above_med = as.factor(\n    ifelse(crim > median(crim), \"Yes\", \"No\")))\n\n# Fit the model\nzn.fit <- glm(crim_above_med ~ zn, data = boston, family = binomial)\nsummary(zn.fit)\n\n\n\nCall:\nglm(formula = crim_above_med ~ zn, family = binomial, data = boston)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4319  -1.4319   0.4634   0.9427   1.8532  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.58076    0.10727   5.414 6.17e-08 ***\nzn          -0.09545    0.01349  -7.075 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 701.46  on 505  degrees of freedom\nResidual deviance: 559.53  on 504  degrees of freedom\nAIC: 563.53\n\nNumber of Fisher Scoring iterations: 6\n\nThe zn.fit summary above will show the coefficients and p-values; the $coef aspect of the summary will show more detail.\npredict() can then be fed the fit to make predictions. The argument type = \"response\" will tell it to output probabilities “of the form \\(P(Y = 1|X)\\)” from the data that was used in the fit, also described as the training data. It returns a vector of probabilities for every data element.\n\n\nShow code\n\n# Create the probability vector from the fit\nzn.probs <- predict(zn.fit, type = \"response\")\nhead(zn.probs) %>% pander()\n\n\n1\n2\n3\n4\n5\n6\n0.2428\n0.6412\n0.6412\n0.6412\n0.6412\n0.6412\n\nThe responses can be converted into categories and turned into a confusion matrix to show how the regression did with the training data.\n\n\nShow code\n\n# Convert the probability vector into yes-or-no predictions\nzn.pred <- ifelse(zn.probs > 0.5, \"Yes\", \"No\")\n# Display a confusion matrix\ntable(zn.pred, boston$crim_above_med) %>% pander()\n\n\n \nNo\nYes\nNo\n119\n15\nYes\n134\n238\n\nThe overall success rate can be calculated with mean(), comparing each predicted value to the actual value:\n\n\nShow code\n\n# See how many of the predictions match the actual value\nmean(zn.pred == boston$crim_above_med)\n\n\n[1] 0.7055336\n\nThe model can then be run on test data that is held out from the main set, with the expectation that it won’t do quite as well as the training data. The efficacy can then be assessed in the same way, and if it’s not high, we can return to the model and try to find other forms that make better predictions - or perhaps conclude that we can’t make a good prediction at all.\nLinear Discriminant Analysis (LDA)\nThe simplest use case for LDA is when there are more than two possible response classes; logistic regression gives the probability that an observation is in a class A or B, but it can only handle two classes.\nLDA also gives better results when “there is substantial separation between the two classes”, or where the sample size is small and the distribution of the predictors is approximately normal in each of the classes.\n\nHow do we determine this?\nResampling methods\nResampling methods are used to refine and test models against training data by repeatedly testing against different subsets within the data.\nCross-validation can be used to estimate the test error rate of a method (model assessment), and bootstrap is mostly to measure the accuracy of a parameter or of a method.\nCross-validation: validation set approach\nThis approach randomly splits the data into two sets: a training set and a validation or test set. Size ratios seem to be on the order of between 50-50 to 3:1 or 4:1 training-to-test data. The model is fit against the training data, and then predict() is used to predict the results for the validation data; the results of predict can then be compared against the actual classifications in the validation data to compute the error rate.\nThis process can be run repeatedly with different splits.\nThe general process is:\nRandomly split the dataset into training and validation sets\nFit the model to the training data\nPredict the outcomes of the model on the validation data\nConvert the predictions into categorical variables that match the field being predicted (like Yes/No, etc)\nThe error rate is the proportion of correct predictions in the validation data.\nThe code below runs 10 iterations and accumulates the results into a results data frame.\n\n\nShow code\n\nset.seed(1235)\ntrain_pct <- .75\n\np5_3a.results <- data.frame(iteration = integer(),\n                           error_rate = double())\n\nfor (i in 1:10) {\n  p5_3a.training <- rep(FALSE, nrow(Default))\n  p5_3a.training[sample(nrow(Default), nrow(Default) * train_pct)] <- TRUE\n  p5_3a.validation <- !p5_3a.training\n\n  p5_3a.fit <-\n    glm(\n      formula = default ~ income + balance,\n      family = binomial,\n      data = Default,\n      subset = p5_3a.training\n    )\n\n  p5_3a.probs <- predict(p5_3a.fit, Default[p5_3a.validation, ], type = \"response\")\n  p5_3a.pred <- ifelse(p5_3a.probs > 0.5, \"Yes\", \"No\")\n  p5_3a.validation_error_rate <-\n    mean(p5_3a.pred == Default[p5_3a.validation, ]$default)\n  p5_3a.validation_error_rate_print <-\n    round(p5_3a.validation_error_rate, 2)\n  \n  p5_3a.results[i,] <- data.frame(\n    iteration = i,\n    error_rate = 1 - p5_3a.validation_error_rate\n  )\n}\n\n\n\nCross-validation: leave-one-out (LOOCV)\nThis method is similar to the validation set approach, except that the validation set is only one element, and the training set is the remaining \\(n-1\\) observations. It is run automatically \\(n\\) times and returns an average of the test errors.\nThe process is simpler than the validation set approach:\nFit the model to the entire data set\nRun cv.glm on the fit\nThe delta factor of the return data is the cross-validation error rate for the test data.\nCode from the book:\n\n\nShow code\n\nlibrary(boot)\nglm.fit <- glm(mpg ~ horsepower, data = Auto)\ncv.error <- cv.glm(Auto, glm.fit)\ncv.error$delta\n\n\n\nCross-validation: k-fold\nK-fold cross-validation is like leave-one-out cross-validation but with a larger test set; it splits the data up into \\(k\\) segments and runs the cross-validation on each. It may give better results if run repeatedly and averaged, as with the validation set approach. It is faster than LOOCV, obviously, because it uses less data.\nThe process is the same as LOOCV, except a parameter such as K = 10 is added to the cv.glm() call. 5 to 10 is a common range for values of \\(k\\).\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-02T15:19:20-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ps753-hw5/",
    "title": "Homework 5: Constraint and Brokerage in the Florentine Wedding dataset",
    "description": "Examining additional centrality measures in the Florentine network",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-16",
    "categories": [
      "r",
      "social networks"
    ],
    "contents": "\n\nContents\nBurt’s constraint\nBrokerage\n\n\n > Academics > PS753 > Homework 5: Constraint and Brokerage in the Florentine Wedding dataset\n\n\n\nShow code\n\ndata(\"florentine\", package = \"ergm\")\nntwk_stat <- flomarriage\n\ndata(flo)\nntwk_ig <- graph_from_adjacency_matrix(flo)\n\n\n\nBuilding on last week’s look at basic network centrality attributes in the Florentine Wedding dataset, we’re going to look at two new measurements of centrality: brokerage and constraints.\n(Note: these two concepts begin to break the “use either one” conclusion when choosing statnet or igraph; brokerage is a function of statnet, and constraints are a function of igraph.)\n\n\nShow code\n\nntwk_ig.nodes <- data.frame(\n    name      = V(ntwk_ig)$name,\n    totdegree = igraph::degree(ntwk_ig, loops = FALSE),\n    indegree  = igraph::degree(ntwk_ig, mode = \"in\", loops = FALSE),\n    outdegree = igraph::degree(ntwk_ig, mode = \"out\", loops = FALSE),\n    eigen     = centr_eigen(ntwk_ig, directed = T)$vector,\n    bonanich  = power_centrality(ntwk_ig),\n    centr_clo = igraph::closeness(ntwk_ig),\n    centr_btw = igraph::betweenness(ntwk_ig, directed = FALSE),\n    # igraph only\n    burt      = constraint(ntwk_ig)\n)\n# Network-level measures:\n#   closeness centralization\nntwk_ig.centr_clo = centr_clo(ntwk_ig)$centralization\n#   betweenness centralization\nntwk_ig.centr_btw = centr_betw(ntwk_ig, directed = FALSE)$centralization\n\n\nntwk_stat.nodes <- data.frame(\n    name      = ntwk_stat %v% \"vertex.names\",\n    totdegree = sna::degree(ntwk_stat),\n    indegree  = sna::degree(ntwk_stat, cmode = \"indegree\"),\n    outdegree = sna::degree(ntwk_stat, cmode = \"outdegree\"),\n    eigen     = sna::evcent(ntwk_stat, ignore.eval = TRUE),\n    bonanich  = sna::bonpow(ntwk_stat),\n    centr_clo = sna::closeness(ntwk_stat, gmode = \"graph\",\n                               cmode = \"suminvundir\"),\n    centr_btw = sna::betweenness(ntwk_stat, gmode = \"graph\")\n)\n# Network-level measures:\n#   closeness centralization\nntwk_stat.centr_clo = sna::centralization(ntwk_stat.nodes, sna::closeness, \n                                     mode = \"graph\")\n#   betweenness centralization\nntwk_stat.centr_btw = sna::centralization(ntwk_stat.nodes, sna::betweenness,\n                                     mode = \"graph\")\n\n# Statnet-only: Gould-Fernandez Brokerage\n# temp <- data.frame(brokerage(ntwk_stat, cl = ntwk.nodes$orgtype5)$z.nli)\n# ntwk.nodes <- ntwk.nodes %>%\n#   mutate(broker.tot = temp$t,\n#          broker.coord = temp$w_I,\n#          broker.itin = temp$w_O,\n#          broker.rep = temp$b_IO,\n#          broker.gate = temp$b_OI,\n#          broker.lia = temp$b_O)\n\n\n\nBurt’s constraint\nBurt’s constraint can be thought of as a measure of a node’s dependency on other nodes for connections or information. If a node has a small number of connections, or the connections are largely redundant (i.e. a small group of friends without many other connections), that node has a higher constraint than one that has connections to nodes that are less redundant to each other. It provides more pathways, or opportunities, for information or power to flow.\nIt should come as no surprise, given all of the various ways that the Medici are the dominant node in this dataset, that the Medici family has the lowest overall constraint.\nAs a reminder, here is the original network graph of the Florence wedding data with no scaling applied:\n\n\nShow code\n\nset.seed(1235)\nplot(\n  ntwk_ig,\n  layout = layout_components(ntwk_ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nAnd here we scale the nodes by their Burt’s constraint measure:\n\n\nShow code\n\nntwk_ig_b <- ntwk_ig\nntwk_ig_b.nodes <- ntwk_ig.nodes\nntwk_ig_b.nodes$burt <- ifelse(is.na(ntwk_ig_b.nodes$burt), 1, ntwk_ig_b.nodes$burt)\n\nntwk_ig_b.nodes$burt_inv <- 5 * (1 - ntwk_ig_b.nodes$burt)\nV(ntwk_ig_b)$size <- ntwk_ig_b.nodes[,\"burt_inv\"] * 5\nV(ntwk_ig_b)$label.cex <- ntwk_ig_b.nodes[,\"burt_inv\"] / 4\nV(ntwk_ig_b)$label.cex <- pmax(V(ntwk_ig_b)$label.cex, 1)\n\n# Ensure minimum node size\nV(ntwk_ig_b)$size <- pmax(V(ntwk_ig_b)$size, 1)\n\nburts_labels <- sprintf(\"%s\\n%s\", V(ntwk_ig_b)$name,\n                            # Omit leading \"0\" can't be done in sprintf\n                            substr(as.character(round(\n                              ntwk_ig_b.nodes$burt, 3\n                            )), 2, 10))\nset.seed(1235)\nplot(\n  ntwk_ig_b,\n  layout = layout_components(ntwk_ig_b),\n  vertex.label = burts_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence families by Burt's constraint\"\n)\n\n\n\n\nOnce again, we see the clear dominance of the Medici family in this measure. Note that all of the pendant nodes (Lamberteschi, Ginori, Pazzi and Acciaiuoli) have maximal constraints of 1; they are completely constrained by their single connections, by definition. The Pucci family’s constraint is undefined, as it is a disconnected node.\nThe Medici have the lowest constraints of any family, being well-connected to other well-connected nodes with some spread among them. It is interesting to note that the Guadagni family is not far behind, though, at 0.25 to 0.21 for the Medici. They are connected to 3 well-connected nodes besides the Lamberteschi, and the Bischeri node in particular gives them indirect access to the Strozzi and Peruzzi nodes, besides the Albizzi and Tornabouni nodes that they share with the Medici. It can be seen from this that the Bischeri node is a strength for the Guadagni family in much the same way that the Barbadori node is a strength of the Medici.\nBrokerage\nWith Gould-Fernandez Brokerage, we begin to look at the ways in which nodes mediate connections between other groups of nodes. This requires a directed network, since the direction of flow of access is a crucial aspect of the core concept. There are five roles recognized by Gould-Fernandez Brokerage:\nCoordinator\nmediates contact between two individuals from his or her own group\n\nItinerant broker\nmediates contact between two individuals from a single group to which he or she does not belong\n\nGatekeeper\nmediates an incoming contact from an out-group member to an in-group member\n\nRepresentative\nmediates an outgoing contact from an in-group member to an out-group member\n\nLiaison\nmediates contact between two individuals from different groups, neither of which is the group to which he or she belongs\n\nUnfortunately for the present moment, however, we cannot apply this concept to the Florence wedding data set, because at present there are no subgroups or categories between which to identify brokers! We will need to add data to the set to find a basis for this sort of analysis, and this is where we begin to transition to a research project.\nI am interested in the patronage system in Renaissance Florence, particularly as it applied to artists and scientists, as a measure of influence of the families. We may be able to return to brokerage concepts once we map in data related to this concept.\n\n\n\n",
    "preview": "academics/ps753-hw5/ps753-hw5_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-03-20T21:45:40-04:00",
    "input_file": "ps753-hw5.knit.md",
    "preview_width": 1248,
    "preview_height": 1536
  },
  {
    "path": "academics/ps753-hw3/",
    "title": "Homework 3: The Florentine Wedding dataset",
    "description": "A first-look network analysis",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-09",
    "categories": [
      "r",
      "social networks"
    ],
    "contents": "\n\n > Academics > PS753 > Homework 3: The Florentine Wedding dataset\n\n\n\nShow code\n\ndata(flo)\nntwk.ig <- graph_from_adjacency_matrix(flo)\n\nntwk.vcount <- vcount(ntwk.ig)\nntwk.ecount <- ecount(ntwk.ig)\nntwk.is_bipartite <- is_bipartite(ntwk.ig)\nntwk.is_directed <- is_directed(ntwk.ig)\nntwk.is_weighted <- is_weighted(ntwk.ig)\nntwk.vertex_attr_names <- vertex_attr_names(ntwk.ig)\nntwk.edge_attr_names <- edge_attr_names(ntwk.ig)\nntwk.names <- V(ntwk.ig)$name\nntwk.dyad_census <- igraph::dyad.census(ntwk.ig)\nntwk.triad_census <- igraph::triad_census(ntwk.ig)\nntwk.global_clustering_coef <- transitivity(ntwk.ig, type = \"global\")\nntwk.local_avg_clustering_coef <- transitivity(ntwk.ig, type = \"average\")\nntwk.avg_path_length <- average.path.length(ntwk.ig, directed = T)\nntwk.component_names <- names(igraph::components(ntwk.ig))\nntwk.components_no <- igraph::components(ntwk.ig)$no\nntwk.components_csize <- igraph::components(ntwk.ig)$csize\n\n# distances(ntwk.ig,\"Acciaiuoli\",\"Strozzi\", weights=NA)\n\nntwk.graph_density <- graph.density(ntwk.ig)\nntwk.graph_density_noloops <- graph.density(ntwk.ig, loops = TRUE)\n\nntwk.avg_degree <- igraph::degree(ntwk.ig)\n\n# Get the degree info and make the data frame\n\nntwk.nodes <- data.frame(\n  name = V(ntwk.ig)$name, \n  degree = igraph::degree(ntwk.ig),\n  indegrees = igraph::degree(ntwk.ig, mode = \"in\", loops = FALSE),\n  outdegrees = igraph::degree(ntwk.ig, mode = \"out\", loops = FALSE)\n  )\n# ntwk.nodes[ntwk.nodes$name %in% c(\"Medici\", \"Strozzi\"),]\n\n# Get degree centralization\n\nntwk.indegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"in\")$centralization\nntwk.outdegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"out\")$centralization\nntwk.total_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"total\")$centralization\n\n\n\n\nFor this assignment, I am looking at the Florentine Wedding dataset by J. F. Padgett, representing intermarriages between prominent families in Florence between 1282 and 1500. This is an area of interest to me as I have some background in fine art, and the Medici family in particular was known for its patronage of some of history’s greatest artists throughout the Renaissance, including Boticelli, Leonardo da Vinci, and Michelangelo. No less than Galileo Galilei himself tutored several generations of Medici children.\nThe great families of this era were frequently fierce rivals, with delicately balanced power structures measured in various spheres of influence; intermarriages were one way that alliances were forged between families in the ceaseless effort to gain political or financial advantage over their competitors.\nThe Medici family was the most prominent in Florence throughout the renaissance. Two other prominent families, the Strozzi and Guadagni, also exercised considerable influence (they were all banking families), and did not intermarry with the Medici, though they were not far removed. This essay looks at the marriage network between the various families as one measure of social power and centrality in Renaissance Florence.\nThe flo dataset in the R network library is an adjacency matrix of 16 families of this era, including the aforementioned. It contains the 16 nodes of the families, and 20 unweighted, directed edges between them. The Pucci family is an isolate, making the network disconnected; the remaining nodes form a single component.\nThe families in the dataset, with their total intermarriage count (degree), marriages into each family (indegrees) and marriages out to other families (outdegrees) are:\n\n\nShow code\n\nntwk.nodes[,c(\"degree\", \"indegrees\", \"outdegrees\")]\n\n\n             degree indegrees outdegrees\nAcciaiuoli        2         1          1\nAlbizzi           6         3          3\nBarbadori         4         2          2\nBischeri          6         3          3\nCastellani        6         3          3\nGinori            2         1          1\nGuadagni          8         4          4\nLamberteschi      2         1          1\nMedici           12         6          6\nPazzi             2         1          1\nPeruzzi           6         3          3\nPucci             0         0          0\nRidolfi           6         3          3\nSalviati          4         2          2\nStrozzi           8         4          4\nTornabuoni        6         3          3\n\nA visualization shows each marriage as a line connecting two families:\n\n\nShow code\n\nV(ntwk.ig)$size <- ntwk.nodes[,\"degree\"] * 5\nV(ntwk.ig)$label.cex <- ntwk.nodes[,\"degree\"] / 4\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\nplot(\n  ntwk.ig,\n  layout = layout_in_circle(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nEach node is sized according to its number of connections; it is easy to see from this diagram that the Medici family had the most marriages within the Florence family network, with six. The Strozzi and Guadagni families were next at four apiece.\nA component layout offers a different view that makes distances and paths easier to read:\n\n\nShow code\n\nset.seed(1235)\nV(ntwk.ig)$size <- ntwk.nodes[,\"degree\"] * 3\nplot(\n  ntwk.ig,\n  layout = layout_components(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nFrom this layout, we can see that the Medici, Strozzi and Guadagni never directly intermarried, but were also only separated from each other by one node. With an overall average path length of approximately 2.5, this suggests some calculus and deliberation: not directly joining with their main rivals, but remaining only one degree removed.\nThe overall network density is \\(\\approx\\) 0.16; 20 connections by marriage out of a possible total of 120. Four of the 16 nodes are pendants. We might expect the network density to increase if we looked at more families than the 16 listed here; it is very unlikely that only 16% of families experienced any marriages at all! This would need to be weighed against the prominence of the additional families according to the criteria with which this initial set was selected.\nIt would be interesting to me to look deeper into a bipartite network of connections between the families and whom they patronized, especially the artists and scientists of the era. This would require more data than is present in this set, but could be very revealing about other ways in which the families interacted, competing for social and political influence in the dynamic period of the Renaissance.\n\n\n\n",
    "preview": "academics/ps753-hw3/ps753-hw3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-02-10T12:20:48-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1536
  },
  {
    "path": "academics/ps753-hw4/",
    "title": "Homework 4: Centrality Analysis of the Florentine Wedding dataset",
    "description": "Examining centrality measures in the Florentine network",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-09",
    "categories": [
      "r",
      "social networks"
    ],
    "contents": "\n\nContents\nCloseness Centrality\nBetweenness Centrality\nEigenvector and Bonacich Power\n\n\n > Academics > PS753 > Homework 4: Centrality Analysis of the Florentine Wedding dataset\n\n\n\nShow code\n\ndata(flo)\nntwk.ig <- graph_from_adjacency_matrix(flo)\n\nntwk.vcount <- vcount(ntwk.ig)\nntwk.ecount <- ecount(ntwk.ig)\nntwk.is_bipartite <- is_bipartite(ntwk.ig)\nntwk.is_directed <- is_directed(ntwk.ig)\nntwk.is_weighted <- is_weighted(ntwk.ig)\nntwk.vertex_attr_names <- vertex_attr_names(ntwk.ig)\nntwk.edge_attr_names <- edge_attr_names(ntwk.ig)\nntwk.names <- V(ntwk.ig)$name\nntwk.dyad_census <- igraph::dyad.census(ntwk.ig)\nntwk.triad_census <- igraph::triad_census(ntwk.ig)\nntwk.global_clustering_coef <- transitivity(ntwk.ig, type = \"global\")\nntwk.local_avg_clustering_coef <- transitivity(ntwk.ig, type = \"average\")\nntwk.avg_path_length <- average.path.length(ntwk.ig, directed = T)\nntwk.component_names <- names(igraph::components(ntwk.ig))\nntwk.components_no <- igraph::components(ntwk.ig)$no\nntwk.components_csize <- igraph::components(ntwk.ig)$csize\n\n# distances(ntwk.ig,\"Acciaiuoli\",\"Strozzi\", weights=NA)\n\nntwk.graph_density <- graph.density(ntwk.ig)\nntwk.graph_density_noloops <- graph.density(ntwk.ig, loops = TRUE)\n\nntwk.avg_degree <- igraph::degree(ntwk.ig)\n\n# Get the degree info and make the data frame\n\nntwk.nodes <- data.frame(\n  name = V(ntwk.ig)$name, \n  degree = igraph::degree(ntwk.ig),\n  indegrees = igraph::degree(ntwk.ig, mode = \"in\", loops = FALSE),\n  outdegrees = igraph::degree(ntwk.ig, mode = \"out\", loops = FALSE)\n  )\n# ntwk.nodes[ntwk.nodes$name %in% c(\"Medici\", \"Strozzi\"),]\n\n# Get degree centralization\n\nntwk.indegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"in\")$centralization\nntwk.outdegree_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"out\")$centralization\nntwk.total_centralization <- \n  centr_degree(ntwk.ig, loops = FALSE, mode = \"total\")$centralization\n\n\n\nThis week, we’re going to look at various measures of degree centrality in the Florentine Wedding dataset.\nLast week, we looked at some basic network plots that showed the overall structure of the network, sizing the nodes by their individual degrees. One of the primary features of this data set is all of the ways in which it can be shown how dominant the Medici family was in medieval Florence.\n\n\nShow code\n\nV(ntwk.ig)$size <- ntwk.nodes[,\"degree\"] * 5\nV(ntwk.ig)$label.cex <- ntwk.nodes[,\"degree\"] / 4\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\nset.seed(1235)\nV(ntwk.ig)$size <- ntwk.nodes[,\"degree\"] * 3\nplot(\n  ntwk.ig,\n  layout = layout_components(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence interfamily marriages, 1282-1500\"\n)\n\n\n\n\nCloseness Centrality\nCloseness centrality is a measure of how close a node is, via the shortest path, to every other node in the network. It is more revealing in hierarchies, where dominant nodes that are closer to the center of the graph will have, on average, shorter paths to the rest of the node than the nodes at the periphery.\nThe closeness centrality measures of the nodes in the Florentine data may come as a bit of a surprise:\n\n\nShow code\n\ncloseness_centrality <- closeness(ntwk.ig, mode = \"all\")\n# Calculate node size by 1000 x closeness\nV(ntwk.ig)$size <- closeness_centrality * 1000\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 1)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\nset.seed(1235)\ncloseness_labels <- sprintf(\"%s\\n%s\", ntwk.names,\n                            # Omit leading \"0\" can't be done in sprintf\n                            substr(as.character(round(\n                              closeness_centrality, 3\n                            )), 2, 10))\nplot(\n  ntwk.ig,\n  layout = layout_components(ntwk.ig),\n  vertex.label = closeness_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by closeness centrality\"\n)\n\n\n\n\nAlthough careful examination shows that the Medici family is still the dominant node, with the highest closeness centrality score, there is very little variation among all but the disconnected Pucci node. The chief reason for this is that the network itself is very small, and tightly connected. As we saw last week, the average path length for this network is 2.486; the Florentine families were all in close proximity to each other, as might be expected given that one of its primary constraints is geographical, but it is interesting to see how the power of the Medici family did not derive primarily from its closeness centrality, or its position in a hierarchy.\nBetweenness Centrality\nAnother useful measure is betweenness centrality, which is a count of the number of shortest paths between nodes that pass through another node. It can be seen as a measure of importance or control, in that it reflects how well a node is positioned along the optimal path between two other nodes.\n\n\nShow code\n\nbetweenness_centrality <- betweenness(ntwk.ig)\n# Calculate node size by 2 x betweenness\nV(ntwk.ig)$size <- betweenness_centrality / 2\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 10)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\ncolors <- case_when(\n  betweenness_centrality == 0 ~ \"#FFFF33\",\n  betweenness_centrality < 20 ~ \"#FF8033\",\n  TRUE ~ \"#FF0033\"\n)\nset.seed(1235)\nplot(\n  ntwk.ig,\n  vertex.color = colors,\n  layout = layout_components(ntwk.ig),\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by betweenness centrality\"\n)\n\n\n\n\nThis chart uses heatmap colors to indicate the strength of betweenness centrality. The yellow nodes - the network’s pendants and isolate - are yellow, which indicates a betweenness measure of 0. The orange nodes are nodes with 20 or less optimal paths running through them, and the red nodes have more than 20. In fact, the scale had to be forced in this graph to make the smaller-weighted betweenness nodes visible at all compared to the overwhelming dominance of the Medici node, with a betweenness score more than twice that of its nearest rival:\n\n\nShow code\n\nsort(round(betweenness_centrality, 2), decreasing = TRUE)\n\n\n      Medici     Guadagni      Albizzi     Salviati      Ridolfi \n       95.00        46.33        38.67        26.00        20.67 \n    Bischeri      Strozzi    Barbadori   Tornabuoni   Castellani \n       19.00        18.67        17.00        16.67        10.00 \n     Peruzzi   Acciaiuoli       Ginori Lamberteschi        Pazzi \n        4.00         0.00         0.00         0.00         0.00 \n       Pucci \n        0.00 \n\n95 of the 312 optimal paths in the Florence network ran through the Medici. It is easy to imagine many different ways that such centrality could manifest as various forms of power.\nEigenvector and Bonacich Power\nEigenvector centrality is a measure of not just how many alters a node is connected to, but of how well-connected those alters are. A node with a high eigenvector centrality score is connected to significant numbers of other highly central nodes. For this chart, we can return to standard-area scaling (except for slightly inflating the smallest to keep them visible), and we see that once again, the Medici are in a dominant position, well-connected to the other well-connected Florence families:\n\n\nShow code\n\neigen_info <- centr_eigen(ntwk.ig, directed = T)\n\n# Calculate node size by 1000 x closeness\nV(ntwk.ig)$size <- eigen_info$vector * 30\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 5)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\n# colors <- case_when(\n#   betweenness_centrality == 0 ~ \"#FFFF33\",\n#   betweenness_centrality < 20 ~ \"#FF8033\",\n#   TRUE ~ \"#FF0033\"\n# )\nset.seed(1235)\neigen_labels <- sprintf(\"%s\\n%.2f\", ntwk.names, eigen_info$vector)\n\nplot(\n  ntwk.ig,\n  # vertex.color = colors,\n  layout = layout_components(ntwk.ig),\n  vertex.label = eigen_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by eigenvector centrality\"\n)\n\n\n\n\nIt is interesting to look at the Strozzi family in this diagram, which has the second-highest eigenvector centrality score behind the Medici. Recall that in terms of simple degree, the Strozzi and Guadagni families were equal, and in terms of betweenness centrality, the Guadagini family had a significantly higher score than the Strozzi, showing that more optimal / shortest paths between nodes passed through the Guadagini. This could be interpreted as a measure of positional influence and power. However, here we see that the Strozzi’s eigenvector centrality is higher than the Guadagini, and it is plain to see why; the Strozzi are connected to four other families with relatively high eigenvector centrality (the Bischeri, Peruzzi and Castellani), where the Guadagni family is somewhat penalized by its connection to the Lamberteschi pendant node that has no other connections. So, although the Guadagni family was more centrally positioned in terms of betweenness, the Strozzi family’s fewer optimal paths were nonetheless connected to more central families.\nFinally, it is worth examining the Bonacich power connections, which is an interesting measure that in some way inverts the eigenvector concept, proposing that being well-connected to other well-connected nodes does not mean that a node has the ability to influence those nodes, as they are also well-connected, and so therefore there is advantage in being connected to weaker nodes. When viewed purely in terms of potential power dynamics, this has some interesting merit.\n\n\nShow code\n\nbon_info <- power_centrality(ntwk.ig)\n\n# H/T: range01 <- function(x){(x-min(x))/(max(x)-min(x))}\nrange01 <- function(x){(x-min(x))/(max(x)-min(x))}\nscaled_bon_info <- range01(bon_info)\n\n# Calculate node size by 1000 x closeness\nV(ntwk.ig)$size <- scaled_bon_info * 30\n# Ensure minimum node size\nV(ntwk.ig)$size <- pmax(V(ntwk.ig)$size, 5)\n# Base text size on node size\nV(ntwk.ig)$label.cex <- V(ntwk.ig)$size / 15\nV(ntwk.ig)$label.cex <- pmax(V(ntwk.ig)$label.cex, 1)\n\n# colors <- case_when(\n#   betweenness_centrality == 0 ~ \"#FFFF33\",\n#   betweenness_centrality < 20 ~ \"#FF8033\",\n#   TRUE ~ \"#FF0033\"\n# )\nset.seed(1235)\neigen_labels <- sprintf(\"%s\\n%.2f\", ntwk.names, bon_info)\n\nplot(\n  ntwk.ig,\n  # vertex.color = colors,\n  layout = layout_components(ntwk.ig),\n  vertex.label = eigen_labels,\n  edge.arrow.mode = 0,\n  vertex.label.color = \"black\",\n  vertex.shape = \"circle\",\n  vertex.label.font = 2,\n  main = \"Florence nodes by Bonacich centrality\"\n)\n\n\n\n\nThis offers a very different view of the power dynamics in Florence. The Medici are greatly reduced, because their direct connections are all powerful families themselves, and so although they are well-connected by other measures, they may not be in a position of power over those other families. More influential families according to Bonacich would include the Ridolfi and Tornabuoni families, who suddenly become dominant from this perspective. The Bonacich calculation is complex and includes notions of centrality and power, and is not purely a measure of adjacency; see how small the Albizzi’s Bonacich’s score is, even though one of its three connections is to the pendant Ginori family, which is nonetheless in a slightly stronger position according to Bonacich. A fuller investigation of this dynamic would require some careful research.\n\n\n\n",
    "preview": "academics/ps753-hw4/ps753-hw4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-02-28T17:02:18-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1536
  },
  {
    "path": "academics/ps753-useful-r-tidbits/",
    "title": "Useful R tidbits: Social Networks",
    "description": "Tasty R snacks that do useful things in Social Networks",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-08",
    "categories": [
      "r",
      "social networks"
    ],
    "contents": "\n\nContents\nPrelude: finding data to play with\nWeek 1 tutorial cheatsheet\nGetting the basic network descriptors\n\nWeek 2 cheatsheet\nTransitivity\n\nDegree\nIndegree and outdegree calculations\n\nComponents\nGraph density\nNetwork degree centralization\nEigenvector centralization\nBonacich Power Centrality\nDerived and Reflected Centrality\n\nWeek 5: Big Block of Basic Code\nCloseness centrailty\nCloseness centralization\nBetweenness centrality\nBetweenness centralization\n(Burt’s) network constraint\nGould-Fernandez Brokerage\n\n\n\n > Academics > PS753 > Useful R tidbits\n\nThis document contains R snippets pertaining to work in the Social Networks class. There is also one for the Machine Learning class.\nPrelude: finding data to play with\nSince in the class tutorials, the network objects are pre-loaded in the web-based runtime environments, we don’t actually see how to load some of the data sets that the exercises are based on, for running outside of this environment.\nThere are gazillions of ways to get data into R, but for the purposes of playing with networks, there are some packages with pre-existing network data that can be easily loaded. Two of these are igraphdata, which contains igraph-formatted network datasets, and ergm, which contains statnet-compatible network datasets.\nThe datasets in each can be listed with the data command:\n> data(package = \"igraphdata\")\n\nData sets in package ‘igraphdata’:\n\nKoenigsberg              Bridges of Koenigsberg from Euler's times\nUKfaculty                Friendship network of a UK university faculty\nUSairports               US airport network, 2010 December\nenron                    Enron Email Network\nfoodwebs                 A collection of food webs\nimmuno                   Immunoglobulin interaction network\nkarate                   Zachary's karate club network\nkite                     Krackhardt's kite\nmacaque                  Visuotactile brain areas and connections\nrfid                     Hospital encounter network data\nyeast                    Yeast protein interaction network\n\n> data(package = \"ergm\")\n\nData sets in package ‘ergm’:\n\ncohab_MixMat (cohab)     Target statistics and model fit to a hypothetical\n                         50,000-node network population with 50,000 nodes\n                         based on egocent\ncohab_PopWts (cohab)     Target statistics and model fit to a hypothetical\n                         50,000-node network population with 50,000 nodes\n                         based on egocent\ncohab_TargetStats (cohab)\n                         Target statistics and model fit to a hypothetical\n                         50,000-node network population with 50,000 nodes\n                         based on egocent\necoli1 (ecoli)           Two versions of an E. Coli network dataset\necoli2 (ecoli)           Two versions of an E. Coli network dataset\nfaux.desert.high         Faux desert High School as a network object\nfaux.dixon.high          Faux dixon High School as a network object\nfaux.magnolia.high       Goodreau's Faux Magnolia High School as a network\n                         object\nfaux.mesa.high           Goodreau's Faux Mesa High School as a network\n                         object\nflobusiness (florentine)\n                         Florentine Family Marriage and Business Ties Data\n                         as a \"network\" object\nflomarriage (florentine)\n                         Florentine Family Marriage and Business Ties Data\n                         as a \"network\" object\ng4                       Goodreau's four node network as a \"network\"\n                         object\nkapferer                 Kapferer's tailor shop data\nkapferer2 (kapferer)     Kapferer's tailor shop data\nmolecule                 Synthetic network with 20 nodes and 28 edges\nsamplike (sampson)       Cumulative network of positive affection within a\n                         monastery as a \"network\" object\nsamplk1 (samplk)         Longitudinal networks of positive affection\n                         within a monastery as a \"network\" object\nsamplk2 (samplk)         Longitudinal networks of positive affection\n                         within a monastery as a \"network\" object\nsamplk3 (samplk)         Longitudinal networks of positive affection\n                         within a monastery as a \"network\" object\nTo load the statnet “florentine” data from the ergm package:\n> data(\"florentine\", package = \"ergm\")\nThis creates the objects flobusiness and flomarriage, both from this dataset, ready to go in statnet. Note that another version of this dataset is found in the network package flo:\ndata(\"flo\", package = \"network\")\nThis loads the very simple data into a matrix, but it is not yet a network object. To convert it to an igraph object:\nlibrary(network)\nlibrary(igraph)\n\ndata(flo)\nntwk.ig <- graph_from_adjacency_matrix(flo)\nWeek 1 tutorial cheatsheet\n\nigraph\nstatnet\nCount of vertices\nvcount()\n\nCount of edges\necount()\n\nBoth\n\nprint()\nBipartite or single mode?\nis_bipartite()\nprint()\nEdges directed or undirected?\nis_directed()\nprint()\nWeighted? (or binary)\nis_weighted()\nprint()\nvertex attribute\nvector of additional information about nodes in a network\n\nedge attribute\nvector of additional information about the edges in a network\n\nQ: how to get missing edge count in igraph?\n\nigraph\nstatnet\ndisplay vertex attributes\nvertex_attr_names()\nlist.vertex.attributes()\ndisplay edge attributes\nedge_attr_names()\nlist.edge.attributes()\nIn igraph, attributes are accessed via $, using the V and E functions, as in:\nV(karate.ig)$name\nE(karate.ig)$weight\nIn statnet, they are accessed via the %v% and %e% mechanisms, as in:\nkarate.stat %v% \"vertex.names\"\nkarate.stat %e% \"weight\"\nGetting the basic network descriptors\nFirst look at any network is to examine the network size, type (un/directed, un/weighted, bipartite) and available attributes of vertices and edges.\nigraph\nBasic descriptors first:\nvcount(climpref.ig)\n[1] 34\n\necount(climpref.ig)\n[1] 531\n\nis_bipartite(climpref.ig)\n[1] FALSE\n\nis_directed(climpref.ig)\n[1] FALSE\n\nis_weighted(climpref.ig)\n[1] TRUE\nListing vertex and edge attributes:\n> vertex_attr_names(climpref.ig)\n[1] \"name\"                 \"Climate.council\"      \"Klimaallianz\"        \n[4] \"Stiftung.Klimarappen\" \"type3\"                \"type5\"               \n[7] \"dm\"                  \n\n> edge_attr_names(climpref.ig)\n[1] \"weight\"\nAccessing vertex and edge attributes:\n> V(climpref.ig)$name %>% head()\n[1] \"AA\" \"AB\" \"AC\" \"AD\" \"AE\" \"AF\"\n\nE(climpref.ig)$weight %>% head()\n[1] 0.69 0.08 0.04 0.18 0.42 0.21\nstatnet\nThe basic descriptors in statnet are all shown by print():\n> print(flobusiness)\n Network attributes:\n  vertices = 16 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 15 \n    missing edges= 0 \n    non-missing edges= 15 \n\n Vertex attribute names: \n    priorates totalties vertex.names wealth \n\nNo edge attributes\nIt appears, though it’s thinly documented, that these attributes are programmatically accessible through the $gal attribute, as in is_directed <- flobusiness$gal$directed:\n> flobusiness$gal\n$n\n[1] 16\n\n$mnext\n[1] 16\n\n$directed\n[1] FALSE\n\n$hyper\n[1] FALSE\n\n$loops\n[1] FALSE\n\n$multiple\n[1] FALSE\n\n$bipartite\n[1] FALSE\nWeek 2 cheatsheet\nA dyad census will count the reciprocal (mut), asymmetric (asym) and absent (null) dyads, based on directed graphs. In igraph:\nigraph::dyad.census(trade2007.ig)\n$mut\n[1] 11444\n\n$asym\n[1] 3148\n\n$null\n[1] 2244\nIn statnet:\nsna::dyad.census()\n [1]   6225  19035  40611   6442   7044  10097  55355  44966   9200   1876\n[11] 146537  25578  17167  30894 225908 374449\nTriad census is similar:\nigraph::triad_census(trade2007.ig)\nsna::triad.census(gotbook.stat, mode=\"graph\")  # undirected\n      \nsna::triad.census(trade2007.stat)              # directed\n      003   012   102 021D 021U 021C  111D  111U 030T 030C   201  120D\n[1,] 6225 13655 26489 4225 4657 4821 34812 23635 4374  537 97088 15073\n     120U  120C    210    300\n[1,] 7947 14249 136169 283855\nNote that the statnet version gives us information about the types of triads as column names in the matrix it returns. The igraph version also breaks them into the 16 categories, but returns them in a fixed order not detailed in its return value (described in its help documentation).\nThe total number of possible triads in a 298 vertex network is (298 x 297 x 296) / (3 x 2 x 1) - the 3 countdown comes from “triad”. Quads would be (298 x 297 x 296 x 295) / (4 x 3 x 2 x 1).\nTransitivity\nTransitivity is the percentage of potential connected triads - how many are complete. Basic way in igraph is transitivity(). The statnet version is gtrans(), but it only works in directed networks. print() will say whether the network is directed. (Note: in the tutorial, we see that the climate network IS directed, but it returns a different result than igraph: 0.627 vs 0.724. Not clear why. “it is calculating a transitivity score based on an understanding of network structure rooted in hierarchy”)\nLocal transitivity is the local clustering coefficient - how many nodes of an ego are connected to each other. Have to unpack this, but the magic is:\ntransitivity(gotbook.ig, type=\"local\", \n  vids=V(gotbook.ig)[\n  c(\"Petyr Baelish\",\"Jon Snow\", \"Tyrion Lannister\")]) \nThe global clustering coefficient in igraph is\ntransitivity(trade2007.ig, type=\"global\")\n[1] 0.8837142\nThe local coefficient is:\ntransitivity(trade2007.ig, type=\"average\")\n[1] 0.8862707\nNetwork transitivity in statnet is gtrans():\ngtrans(trade2007.stat)\n[1] 0.9993143\n\nigraph\nstatnet\nglobal clustering coefficient\ntransitivity(trade2007.ig,  type=\"global\")\ngtrans(trade2007.stat) (directed only)\nlocal clustering coefficient\ntransitivity(trade2007.ig,  type=\"local\")\n???\naverage local clustering coefficient\ntransitivity(trade2007.ig,  type=\"average\")\n???\nDegree\nigraph::degree() and statnet::degree(), and once again they give different results; igraph includes loops, statnet doesn’t. Force igraph to ignore them with loops = FALSE. “Note that setting diag=TRUE in sna::degree does not guarantee equivalence as statnet only single counts the loops in a directed network, while igraph double-counts the loops.”\nigraph shows the node names, statnet doesn’t.\nGetting the degree of a particular set of nodes in igraph:\nigraph::degree(trade2007.ig, v = V(trade2007.ig)[c(\"China\", \"Canada\", \"United Kingdom\", \"Denmark\")])\n         China         Canada United Kingdom        Denmark \n           364            364            364            362 \nIndegree and outdegree calculations\n\nigraph\nstatnet\nindegree\nigraph::degree(climate.ig,mode=\"in\", loops = FALSE)\nsna::degree(climate.stat, cmode=\"indegree\")\noutdegree\nigraph::degree(climate.ig,mode=\"out\", loops = FALSE)\nsna::degree(climate.stat, cmode=\"outdegree\")\nCode from the tutorial to create data.frames with degree statistics:\n#igraph:\n\ntrade2007.nodes <- data.frame(name = V(trade2007.ig)$name,\n    totdegree = igraph::degree(trade2007.ig, loops = FALSE),\n    indegree = igraph::degree(trade2007.ig, mode = \"in\", loops = FALSE),\n    outdegree = igraph::degree(trade2007.ig, mode = \"out\", loops = FALSE))\n\n#statnet version:\n\ntrade2007.nodes <- data.frame(name = trade2007.stat%v%\"vertex.names\",\n    totdegree = sna::degree(trade2007.stat),\n    indegree = sna::degree(trade2007.stat, cmode = \"indegree\"),\n    outdegree = sna::degree(trade2007.stat, cmode = \"outdegree\"))\nComponents\nShortest path length between 2 nodes: igraph distances() does this.\ndistances(gotbook.ig,\"Petyr Baelish\",\"Robb Stark\")\n# Calculate distance using unweighted edges\ndistances(gotbook.ig,\"Petyr Baelish\",\"Robb Stark\", weights=NA)\n# list shortest paths between 2 nodes\nall_shortest_paths(gotbook.ig,\"Bronn\",\"Varys\", weights=NA)$res\n#find average shortest path for network\naverage.path.length(gotbook.ig,directed=F)\nComponent structure and membership\n# What element are returned by components\nnames(igraph::components(gotbook.ig))\n\n# Number of components\nigraph::components(gotbook.ig)$no \n\n# Size of each component\nigraph::components(gotbook.ig)$csize \n\n# retrieve the index of isolate nodes\n# (nodes with component count of 1 from \"components\" above)\nisolates(gotbook.stat)\n\n# There is no direct command in igraph, but we can do this:\n# Create a list of the degree of each node in the network\ndeg_counts <- igraph::degree(gotbook.ig, loops = FALSE)\n\n# filter and count the nodes with 0 degrees (or any other quantity of interest)\nlength(deg_counts[deg_counts == 0])\n\n# subset vertex.names attribute to get names of isolates\nas.vector(gotbook.stat %v% 'vertex.names')[c(isolates(gotbook.stat))] %>%\n  head()\n##   [1] \"Aegon Frey (Jinglebell)\"         \"Alebelly\"                       \n##   [3] \"Alfyn\"                           \"Allar Deem\"                     \n##   [5] \"Antario Jast\"                    \"Balman Byrch\"    \nGraph density\nNote: network.density() (statnet) ignores edge values “at present”.\n#get network density: igraph\ngraph.density(climate.ig)\n\n## [1] 0.4117647\n\n#get network density: statnet\nnetwork.density(climate.stat)\n\n## [1] 0.399654\nAdding loops = TRUE to graph.density() appears to fix the problem and gets the two packages to agree:\n#get network density without loops: igraph\ngraph.density(climate.ig, loops=TRUE)\n\n## [1] 0.399654\nSO, it’s safest to always do either: - graph.density(climate.ig, loops=TRUE) (igraph), OR - network.density(climate.stat) (statnet)\nIn statnet, we can get network density with loops (nodes connecting to themselves) omitted:\n#get network density without loops: statnet\ngden(climate.stat, diag=FALSE)\n\n## [1] 0.3921569\nNetwork degree centralization\nIn statnet, call centralization() with the degree function and appropriate parameters for degree in the cmode argument:\ncentralization(climate.stat, degree, cmode=\"indegree\")\ncentralization(climate.stat, degree, cmode=\"outdegree\")\ncentralization(climate.stat, degree, cmode=\"freeman\") # default\nCould also call it with other sna functions like betweenness, closeness\nThe igraph version uses centr_degree() and returns an object with several components, of which centralization is one:\ncentr_degree(climate.ig, loops = FALSE, mode = \"in\")$centralization\ncentr_degree(climate.ig, loops = FALSE, mode = \"out\")$centralization\nEigenvector centralization\nstatnet uses evcent() to calculate the eigenvalue centrality score for each node in the network:\nevcent(imf2014.stat, ignore.eval=TRUE))\nEigenvector centrality index for the network:\ncentralization(imf2014.stat, evcent)\nIn igraph, a set of eigenvector-related information is created with centr_eigen():\n# If the network is directed, specify \"directed - T\" - will not auto-detect\neigen_info <- centr_eigen(imf2014.ig, directed = T)\n# Centrality score for node 3:\neigen_info[3]$vector\n# Eigenveector centrality index for the network:\neigen_info$centralization\nThe scores calculated by igraph and statnet are different. We aren’t sure why. It appears that igraph counts incoming ties to calculate eigenvector centrality, and statnet recommends using Bonachic power instead for directed networks.\nBonacich Power Centrality\nigraph:\npower_centrality(imf2014.ig)\nstatnet:\nbonpow(imf2014.stat)\nAgain, there appear to be some inconsistency between igraph and statnet in the calculations, with statnet apparently not incorporating weights and failing on singular matrices.\nDerived and Reflected Centrality\nThere are no library routines for these calculations. Convert the data to a matrix first:\nmat2014 <- as.matrix(as_adjacency_matrix(imf2014.ig, attr=\"weight\"))\n\nTo calculate the proportion of centrality that is received, we first square the adjacency matrix. The diagonal of the adjacency matrix is equal to the the square of node degree. We then divide this diagonal (sqared degree) by total sqaured indegree (calculated by rowSums) to get the proportion of received centrality.\n\n# square the adjacency matrix\nmat2014sq <- t(mat2014) %*% mat2014\n\n# Calculate the proportion of reflected centrality.\nimf2014.nodes$rc <- diag(mat2014sq) / rowSums(mat2014sq)\n\n# freplace missing values with 0\nimf2014.nodes$rc <- ifelse(is.nan(imf2014.nodes$rc), 0, imf2014.nodes$rc)\n\n# Calculate received eigenvalue centrality\nimf2014.nodes$eigen.rc <- imf2014.nodes$eigen * imf2014.nodes$rc\n\nIf total centraltiy is 1, then derived centrality is simply 1 - the proportion of eigenvector centrality due to received centrality.\n\n# Calculate the proportion of derived centrality.\nimf2014.nodes$dc <- 1 - diag(mat2014sq) / rowSums(mat2014sq)\n\n# replace missing values with 0\nimf2014.nodes$dc <- ifelse(is.nan(imf2014.nodes$dc), 1, imf2014.nodes$dc)\n\n# Calculate received eigenvalue centrality\nimf2014.nodes$eigen.dc <- imf2014.nodes$eigen * imf2014.nodes$dc\nWeek 5: Big Block of Basic Code\nBig Blocks of Basic Code to get a bunch of measures of a network:\n# Get the basic stuff we can do all at once with igraph\ncliminfl.nodes <- data.frame(\n    name      = V(climinfl.ig)$name,\n    totdegree = igraph::degree(climinfl.ig, loops=FALSE),\n    indegree  = igraph::degree(climinfl.ig, mode=\"in\", loops=FALSE),\n    outdegree = igraph::degree(climinfl.ig, mode=\"out\", loops=FALSE),\n    eigen     = centr_eigen(climinfl.ig, directed = T)$vector,\n    bonanich  = power_centrality(climinfl.ig),\n    centr_clo = igraph::closeness(climinfl.ig),\n    centr_btw = igraph::betweenness(climinfl.ig, directed = FALSE),\n    # igraph only\n    burt      = constraint(climinfl.ig)\n)\n# Network-level measures:\n#   closeness centralization\ncliminfl.centr_clo = centr_clo(climpref.ig)$centralization\n#   betweenness centralization\ncliminfl.centr_btw = centr_betw(climpref.ig, directed = FALSE)$centralization\n\n# statnet version\ncliminfl.nodes <- data.frame(\n    name      = climinfl.stat %v% \"vertex.names\",\n    totdegree = sna::degree(climinfl.stat),\n    indegree  = sna::degree(climinfl.stat, cmode = \"indegree\"),\n    outdegree = sna::degree(climinfl.stat, cmode = \"outdegree\"),\n    eigen     = sna::evcent(climinfl.stat, ignore.eval = TRUE),\n    bonanich  = sna::bonpow(climinfl.stat),\n    centr_clo = sna::closeness(climinfl.stat, gmode = \"graph\"),\n    centr_btw = sna::betweenness(climinfl.stat, gmode = \"graph\")\n)\n# Network-level measures:\n#   closeness centralization\ncliminfl.centr_clo = centralization(climinfl.stat, FUN = \"closeness\", mode = \"graph\")\n#   betweenness centralization\ncliminfl.centr_btw = centralization(climinfl.stat, FUN = \"betweenness\", mode = \"graph\")\n\n# Statnet-only: Gould-Fernandez Brokerage\ntemp <- data.frame(brokerage(climinfl.stat, cl = climinfl.nodes$orgtype5)$z.nli)\ncliminfl.nodes <- climinfl.nodes %>%\n  mutate(broker.tot = temp$t,\n         broker.coord = temp$w_I,\n         broker.itin = temp$w_O,\n         broker.rep = temp$b_IO,\n         broker.gate = temp$b_OI,\n         broker.lia = temp$b_O)\n\n# Calculated measures not specific to igraph or statnet:\n# Build the derived and reflected centrality (dc/rc) measures\n# \"To calculate the proportion of centrality that is received, we first\n# square the adjacency matrix. The diagonal of the adjacency matrix is\n# equal to the the square of node degree. We then divide this diagonal\n# (squared degree) by total squared indegree (calculated by rowSums) to get\n# the proportion of received centrality.\"\n\nmat_climinfl <- as.matrix(as_adjacency_matrix(climinfl.ig))  # not \" attr='weight'\"\nmat_climinfl_sq <- t(mat_climinfl) %*% mat_climinfl\ncliminfl.nodes$rc <- diag(mat_climinfl_sq) / rowSums(mat_climinfl_sq)\n# replace missing values with 0\ncliminfl.nodes$rc <- ifelse(is.nan(climinfl.nodes$rc), 0, climinfl.nodes$rc)\ncliminfl.nodes$dc <- 1 - climinfl.nodes$rc\n\n# Build the derived and reflected eigenvector measures\ncliminfl.nodes$eigen.rc <- climinfl.nodes$eigen * climinfl.nodes$rc\ncliminfl.nodes$eigen.dc <- climinfl.nodes$eigen * climinfl.nodes$dc\nCloseness centrailty\nFrom text: “The closeness centrality of a node is defined as the sum of the geodesic distances between that node and all other nodes in a network.”\nBoth are called in the above blocks, with sna::closeness() or igraph::closeness() on the respective network object.\nFrom igraph::closeness help: “Closeness centrality measures how many steps is required to access every other vertex from a given vertex.”\nigraph and statnet have very different implementations, with options that have to be carefully set.\nigraph\nigraph uses inverse closeness.\nFor directed networks, use mode=(\"in\", \"out\", \"all\", \"total\"), describing the path type; in is paths to a vertex, out is paths from a vertex. Undirected networks ignore this parameter. It will use the “weight” edge attribute automatically if it’s there, or can be overriden with something else.\nstatnet\nMust specify gmode (type of graph) as graph (undirected) or digraph (directed, default). cmode (type of closeness centrality being measured) is one of: directed, undirected (both standard closeness), suminvdir (directed case) and suminvundir (undirected case), and gil-schmidt for that. The suminv options correspond to igraph’s default inversion, though they’re still calculated slightly differently, so they’re generally preferred.\nstatnet ignores the edge weights by default; ignore.eval = FALSE to use them, according to the documentation, but the results appear not to use them.\nsna::closeness(climpref.stat, gmode=\"graph\", cmode=\"suminvundir\", ignore.eval=FALSE))\nCloseness centralization\nCloseness centralization is the network-level measure of the closeness centrality node-level measure.\ncliminfl.centr_clo = centr_clo(climpref.ig)$centralization\ncliminfl.centr_clo = centralization(climinfl.stat, FUN = \"closeness\", mode = \"graph\")\nBetweenness centrality\nBetweenness centrality is the node-level measure of the number of geodesics (shortest path between two nodes) on which a node sits. A high betweenness centrality measure means a node is on many shortest-paths, suggesting a measure of influence or power.\nigraph::betweenness(climpref.ig, directed = FALSE)\nsna::betweenness(climpref.ig, gmode = \"graph\")\nThe igraph version directed argument, for whether or not direction should be considered, defaults to TRUE; it might be wondered why it doesn’t read the directedness of the graph as a default, but oh well.\nThe statnet version would use gmode of digraph for a directed network, and cmode for a variant undirected form; see ?sna::betweenness for more. Statnet appears to use weights by default; weights = NA disables weights in igraph.\nBetweenness centralization\nThe network-level measure of betweenness centralization represents Freeman centralization, at least according to the statnet documentation.\nThis is a “a measure of how central its most central node is in relation to how central all the other nodes are”.\ncliminfl.centr_btw = centr_betw(climpref.ig, directed = FALSE)$centralization\ncliminfl.centr_btw = centralization(climpref.stat, FUN = \"betweenness\", mode = \"graph\")\n(Burt’s) network constraint\nigraph-only function measuring a node’s connection redundancy from 0 (none) to 1. Uses weights.\nconstraint(climinfl.ig)\nGould-Fernandez Brokerage\n(Statnet-only)\nFrom ?brokerage(): “Gould and Fernandez (following Marsden and others) describe brokerage as the role played by a social actor who mediates contact between two alters.”\nRequires a directed network with a vertex attribute used for grouping. Returns a structure with a lot of information is returned; tutorial refers mainly to znli containing the following roles:\nprefix\nRole\nAction\nPath\nw_I\nCoordinator\nmediates contact between two individuals from his or her own group.\nA -> A -> A\nw_O\nItinerant broker\nmediates contact between two individuals from a single group to which he or she does not belong.\nA -> B -> A\nb_{OI}\nGatekeeper\nmediates an incoming contact from an out-group member to an in-group member.\nA -> B -> B\nb_{IO}\nRepresentative\nmediates an outgoing contact from an in-group member to an out-group member.\nA -> A -> B\nb_O\nLiaison\nmediates contact between two individuals from different groups, neither of which is the group to which he or she belongs.\nA -> B -> C\nt\nTotal\nTotal (cumulative) brokerage role occupancy\n(Any two)\nbrokerage(climinfl.stat, cl = climinfl.nodes$orgtype5)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-20T19:55:34-04:00",
    "input_file": "ps753-useful-r-tidbits.knit.md"
  },
  {
    "path": "academics/ms797-useful-r-tidbits/",
    "title": "Useful R tidbits: Machine Learning",
    "description": "Tasty R snacks that do useful things in Machine Learning",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-06",
    "categories": [
      "r",
      "machine learning"
    ],
    "contents": "\n\nContents\nChapter 3: Linear Regression\nnames(): see the attributes of a model\ncoef(): see the model’s coefficients\nconfint(): see confidence intervals for a model\npredict(): see confidence OR prediction intervals for a model\narranging plots in grids\n\nEVERYTHING BELOW THIS LINE\nMultiple Linear Regression\nInteraction Terms\nNon-linear Transformations of the Predictors\nQualitative Predictors\n\n\n\n > Academics > MS797 > Useful R tidbits\n\nThis document contains R snippets pertaining to work in the Machine Learning class. There is also one for the Social Networks class.\nMost of this comes directly from the resources from the 2nd edition of ISLR, either direct quotes or things that build on them. The Lab section of each chapter contains useful walkthroughs and code illustrating the chapter’s key concepts, and there’s always something to learn about R and how different people use it.\n\nEverything here is, unless otherwise marked, either the work of, or derivative of the work of, the original authors of the book.\nChapter 3: Linear Regression\n\n\n\nnames(): see the attributes of a model\nnames() is a nice, simple way to see the attributes of a linear model:\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(): see the model’s coefficients\ncoef() is a convenient way to get the coefficients attribute:\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nNote: this is equivalent to lm.fit$coefficients, but slightly easier on the eyes.\nconfint(): see confidence intervals for a model\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\nNote: levels can be added with the level parameter (the default is 0.95):\n\n                  5 %       95 %\n(Intercept) 33.626697 35.4809847\nlstat       -1.013877 -0.8862212\n\npredict(): see confidence OR prediction intervals for a model\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\nNote: prediction intervals are defined on p. 82; in brief, it is the interval in which we are \\(X\\)% certain that any future observation will fall.\nNote: the data.frame() call in the above parameter lists produce a nifty little data frame that is used by predict to identify the column to do calculations on, and the desired levels:\n\n  lstat\n1     5\n2    10\n3    15\n\nNote: the book uses the base R graphics:\n\n\n\nThe tidyverse/ggplot version of this is:\n\n\n\n…a bit more work, but more modern.\narranging plots in grids\nThe book uses base R, again, with par() and mfrow() to arrange plots:\n\n\n\nggplot uses facet() to arrange plots from the same data set in grids.\nPlots from different data sets need additional packages to be combined; one option is the cowplot library and the plot_grid() function.\nEVERYTHING BELOW THIS LINE\nis direct paste from the book’s sources which I have not yet written in my own words or added anything to. Stay tuned.\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\n\n375 \n375 \n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\nMultiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  < 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: < 2.2e-16\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\n    crim       zn    indus     chas      nox       rm      age \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 \n     dis      rad      tax  ptratio    lstat \n3.954037 7.445301 9.002158 1.797060 2.870777 \n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  < 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: < 2.2e-16\n\nAlternatively, the update() function can be used.\n\n\n\nInteraction Terms\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age.\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n\nNon-linear Transformations of the Predictors\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\nQualitative Predictors\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age\n1  9.50       138     73          11        276   120       Bad  42\n2 11.22       111     48          16        260    83      Good  65\n3 10.06       113     35          10        269    80    Medium  59\n4  7.40       117    100           4        466    97    Medium  55\n5  4.15       141     64           3        340   128       Bad  38\n6 10.81       124    113          13        501    72       Bad  78\n  Education Urban  US\n1        17   Yes Yes\n2        10   Yes Yes\n3        12   Yes Yes\n4        14   Yes Yes\n5        13   Yes  No\n6        16    No Yes\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n\n",
    "preview": "academics/ms797-useful-r-tidbits/ISLR-cover.png",
    "last_modified": "2022-02-08T22:19:53-05:00",
    "input_file": {},
    "preview_width": 750,
    "preview_height": 1263
  },
  {
    "path": "academics/ms797-glossary/",
    "title": "MS797 Glossary",
    "description": "Glossary for MS797 coursework",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-01-30",
    "categories": [
      "machine learning",
      "glossary"
    ],
    "contents": "\n\nContents\nChapter 2\nChapter 3\nChapter 4\nChapter 5\n\n\n > Academics > MS797 > Glossary\n\nPage numbers in parenthesis after terms, from ISLR 2nd edition. Non-page numbers indicate other sources; “biostats” references material from Biostatistics 690Z (Health Data Science: Statistical Modeling), fall 2021.\nChapter 2\ninput variable (15)\nalso predictor, independent variable, feature; usually written \\(X_1, X_2\\), etc. The parameter or parameters we are testing to see if they are related to or affect the output.\n\noutput variable (15)\nalso response, dependent variable; usually written \\(Y\\). The outcome being measured.\n\nerror term (16)\n\\(\\epsilon\\) in the equation\\[Y = f(X) + \\epsilon\\] a random quantity of inaccuracy, independent of X and with mean 0.\n\nsystematic (16)\n\\(f\\) in the equation\\[Y = f(X) + \\epsilon\\] the function that describes the (systematic) information \\(X\\) provides about \\(Y\\). This plus the error term equals \\(Y\\).\n\nreducible error (18)\nThe amount of the error \\(\\epsilon\\) that could be eliminated by improving our estimator \\(\\hat{f}\\); the difference between \\(\\hat{f}\\) and \\(f\\). This book and course is mostly about ways to minimize the reducible error.\n\nirreducible error (18)\nThe amount of \\(\\epsilon\\) that could not be reduced even if \\(f\\) was a perfect estimator of \\(Y\\). Always greater than 0. Could be due to hidden variables in \\(\\epsilon\\), or random fluctuations in Y, like a measure of “[a] patient’s general feeling of well-being on that day”.\n\nexpected value (19)\naverage value of an expected measure.\n\ntraining data (21)\ndata used to develop the model for estimating \\(f\\).\n\nparametric methods (21)\nA model based on one or more input parameters, that yields a value for Y, as in: \\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\] \\[Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\] \\[\\text{income} \\approx \\beta_0 + \\beta_1 \\times \\text{education} + \\beta_2 \\times \\text{seniority}\\] This creates a predictive, inflexible model which usually does not match the true \\(f\\), but which has advantages of simplicity and interpretability. It can be used to predict values for \\(Y\\) based on its parameters, or inputs. Linear and logistic regression are parametric.\n\nnon-parametric methods (23)\nmethods that do not attempt to estimate \\(f\\). More flexible and have the potential to very closely match observations, but with the risk of overfitting the data and increasing the variance of subsequent observations. They require much more data than parametric models, and may be difficult to interpret, K-Nearest Neighbor and Support Vector Machines are non-parametric.\n\nprediction (26)\nseeking to guess the value of an response variable \\(y_i\\) given a set of observations and a predictor \\(f\\).\n\ninference (26)\na model that seems to better understand the relationship between the response and the predictors.\n\nsupervised learning (26)\na category of model that allows us to guess a \\(y_i\\) response to a set of predictor measurements \\(x_i, i = 1, \\dots, n\\).\n\nunsupervised learning (26)\na category of model in which there are observations/measurements \\(x_i, i = 1, \\dots, n\\), but no associated response \\(y_i\\). Linear regression cannot be used because there is no response variable to predict.\n\ncluster analysis (27)\nin unsupervised learning, a statistical method for determining whether a set of observations can be divided into “relatively distinct groups,” looking for similarities within the groups. (Topic modeling may be an example of this.)\n\nquantitative variables (28)\nnumeric values; age, height, weight, quantity. Usually the response variable type for regression problems.\n\nqualitative variables (28)\nalso categorical: values from a discrete set. Eye color, name, yes/no. Usually the response variable type for classification problems.\n\nregression problems (28)\nproblems with quantitative response variables. Given predictors foo, bar, and baz, how big is the frob?\n\nclassification problems (28)\nproblems with qualitative response variables. Given predictors foo, bar, and baz, is the outcome likely to be a frob, a frib or a freeb?\n\nmean squared error (MSE) (29)\nthe average squared error for a set of observations: \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2\\] MSE is small if the predicted responses are close to the true responses, and larger as it becomes less accurate; computed from training data, and Gareth et al. suggest it should be called training MSE.\n\nvariance (34)\n“the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set” More practically: the average of squared differences from the mean, often expressed as \\(\\sigma^2\\), where \\(\\sigma\\) (or the square root of the variance) is the standard deviation per StatQuest: “the difference in fits between data sets” (like training and test)\n\nbias (35)\n“the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model”, as in the error from the (presumed) linearity of a regression against non-linear data whose complexity it does not capture. More flexible models increase variance and decrease bias. per StatQuest: “The inability for a machine learning method (like linear regression) to capture the true relationship is called bias” - a straight line trying to model a curved separation in classes will never get it right and always be biased\n\nbias (65)\nin an estimator, something that systematically misses the true parameter; for an unbiased estimator, \\(\\hat{\\mu} = \\mu\\) when averaged over (huge) numbers of observations\n\nbias-variance trade-off (36)\nThe tension in seeking the best model for the data between missing the true \\(f\\) with an overly simple (biased) model, vs. an overfitted model with too much variance from mapping too closely to test data.\n\nerror rate (37)\nIn classification, the proportion of classifications that are mistakes. \\[\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\neq \\hat{y}_i)\\] \\(I\\) is 1 if \\(y_i \\neq \\hat{y}_i\\) - if the guess for any given \\(y\\) is wrong. The error rate is the percentage of incorrect classifications. Also the training erorr rate.\n\nindicator variable (37)\n\\(I\\) in the error rate definition above; a logical variable indicating the presence or absence of a characteristic or trait (such as an accurate classification).\n\ntest error rate (37)\nlike the training error rate but applied to the test data. Uses \\(\\text{Ave}\\) instead of sum notation: \\[\\text{Ave}(I(y_0 \\neq \\hat{y}_0))\\] \\(\\hat{y}_0\\) is the predicted class label from the classifier.\n\nconditional probability (37)\nThe chance that \\(Y = j\\) given an observed \\(x_0\\), as in the Bayes classifier: \\[\\text{Pr}(Y = j|X = x_0)\\] In a two-class, yes/no classifier, we decide based on whether \\(\\text{Pr}(Y = j|X = x_0)\\) is \\(> 0.5\\), or not. Note that \\(Y\\) is the class, as in “ham”/“spam”, not a \\(y\\)-axis coordinate.\n\nBayes decision boundary (38)\na visual depiction of the line of 50% probability dividing (exactly two?) classes in a two-dimensional space\n\nBayes error rate (38)\nthe expected (average) probability of classification error over all values of X in a data set. \\[1 - E(\\underset{j}{maxPr}(Y = j|X))\\] The \\(\\underset{j}{maxPr}\\) whichever of the \\(j\\) classes has the highest probability for any given value of \\(X\\). Again, \\(Y\\) is not a y-axis coordinate of a two-dimensional space, it’s the class of the classification: “yes”/“no”, “ham”/“spam”, “infected”/“not infected”. Also: “The Bayes error rate is analogous to the irreducible error, discussed earlier.”\n\nK-nearest-neighbors (KNN) (39)\na classifier that assigns a class Y to an observation based on the population proportions of its nearest neighbors; a circular “neighborhood” on a two-dimensional plot. It looks at actual data points that have been classified, and asks what any given non-classified point would be classified as based on its nearest neighbors.\n\nChapter 3\nSynergy effect / interaction effect (60)\nwhen two or more predictors affect each other as well as the outcome; when 50k each in TV or radio ads give different results than 100k in either one\n\nSimple linear regression\nthe simplest model, predicting \\(Y\\) from a single predictor \\(X\\). \\[Y \\approx \\beta_0 + \\beta_1X\\] \\(\\approx\\) = “is approximately modeled as”\n\nleast squares (61)\nthe most common measure of closeness of a regression line to its data points, the sum of squares of the distances between the points and the closest point on the line (directly above or below)\n\nresidual (61)\nthe difference between \\(y_i\\) and \\(\\hat{y}_i\\), also \\(e_i\\); the difference between the \\(i\\)th response variable and the \\(i\\)th response variable predicted by the model\n\nresidual sum of squares (RSS) (62)\nthe sum of the squared residuals for each point on the regression line \\[\\text{RSS} = e_1^2 + e_2^2 + \\dots + e_n^2\\] Formulas for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are on p. 62\n\nintercept (\\(\\beta_0\\)) (63)\nthe expected value of \\(Y\\) when \\(X = 0\\)\n\nslope (\\(\\beta_1\\)) (63)\nthe average increase in \\(Y\\) associated with a one-unit increase in \\(X\\)\n\nerror term (\\(\\epsilon\\)) (63)\nwhatever we missed with the model, due to the true model not being linear (it almost never is), measurement error, or other variables that cause variation in \\(Y\\)\n\npopulation regression line (63)\n“the best linear approximation to the true relationship between \\(X\\) and \\(Y\\)” \\[Y = \\beta_0 + \\beta_1X + \\epsilon\\] least squares line (63)\n\nthe regression line made of the least-squares estimates for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nstandard error (SE) (65)\nthe average amount that an estimate \\(\\hat{\\mu}\\) (sample mean) differs from the actual value of \\(\\mu\\) (population mean) \\[\\text{Var}(\\hat{\\mu}) = \\text{SE}(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n} (\\text{also} = \\frac{\\sigma}{\\sqrt{n}})\\] \\(\\sigma\\) is “the standard deviation of each of the realizations \\(y_i\\) of \\(Y\\). Since \\(\\sigma^2\\) is divided by \\(n\\), the standard error shrinks as observations increase. It represents the amount we would expect means of additional samples to”jump around\" simply due to random chance and the limitations of the model’s accuracy.1\n\nresidual standard error (RSE) (66)\nthe estimate of \\(\\sigma\\) \\[\\text{RSE} = \\sqrt{RSS / (n-2)}\\]\n\nconfidence interval (66)\na range of values within which we have a measured probability (often 95%) of containing the true value of the parameter; a 95% confidence interval in linear regression takes the form \\[\\hat{\\beta_1} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_1})\\]\n\nt-statistic (67)\nthe number of standard deviations that \\(\\hat{\\beta_1}\\) is away from \\(0\\). \\[t = \\frac{{\\hat{\\beta_1}} - 0}{\\text{SE}(\\hat{\\beta_1})}\\] For there to be a relationship between \\(X\\) and \\(Y\\), \\(\\hat{\\beta_1}\\) has to be nonzero (i.e. have a slope). The standard error (SE) of \\(\\hat{\\beta_1}\\) (in the denominator above) measures its accuracy; if it is small, then \\(t\\) will be larger, and if it is large, then \\(t\\) will be smaller. \\(t\\) is around 2 for a p-value of 0.05 (actually about 1.96, as 2 standard deviations is 95.45% of a normal distribution), and around 2.75 for a p-value of 0.01.\n\np-value (67)\nthe probability of observing a value greater than \\(|t|\\) by chance.\n\nmodel sum of squares (MSS) (biostats)\nAlso sometimes ESS, “explained sum of squares”: the total variance in the response \\(Y\\) that can be accounted for by the model \\[\\text{MSS} = \\sum(\\hat{y_i} - \\bar{y})^2\\]\n\nresidual sum of squares (RSS) (biostats)\nthe total variance in the response \\(Y\\) that cannot be accounted for by the model \\[\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\\] also \\[\\text{RSS} = e_i^2 + e_2^2 + \\dots + e_n^2\\] or \\[\\text{RSS} = (y_1 - \\hat{\\beta_0} - {\\hat{\\beta_1}x_1})^2 + (y_2 - \\hat{\\beta_0} - {\\hat{\\beta_1}x_2})^ + \\dots + (y_n - \\hat{\\beta_0} - {\\hat{\\beta_1}x_n})^2\\]\n\ntotal sum of squares (TSS) (70)\nthe total variance in the response \\(Y\\); the total variability of the response about its mean \\[\\text{TSS} = \\sum(y_i - \\bar{y})^2\\] compare with RSS, the amount of variability left unexplained after the regression. TSS - RSS is the amount of variability (or error) explained by the regression (MSS).\n\nNOTE: there is a nice visual here on stackexchange; if anybody knows how to tell Zotero to use a custom bibtex citation entry over the ones it generates, please let me know so I can integrate it better here :frown:\n\\(R^2\\) statistic (70)\nthe proportion of variance in \\(Y\\) explained by \\(X\\), a range from 0 to 1 \\[R^2 = \\frac{\\text{TSS - RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\\] \\(R^2\\) values close to 1 indicate a regression that explains a lot of the variability in the response, and a stronger model. A value close to 0 indicates that the regression doesn’t explain much of the variability.\n\ncorrelation (70)\na measure of the linearity of the relationship between \\(X\\) and \\(Y\\); values close to 0 indicate weak-to-no relationship, values near 1 or -1 indicate strong positive or negative correlation \\[\\text{Cor(X, Y)} = {\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})(y_{i}-{\\bar {y}})}{{\\sqrt {\\sum _{i=1}^{n}(x_{i}-{\\bar {x}})^{2}}}{\\sqrt {\\sum _{i=1}^{n}(y_{i}-{\\bar {y}})^{2}}}}}\\]\n\nstandard linear regression model (72)\nThe modal used for standard linear models, used to interpret the the effect on \\(Y\\) of a one-unit increase in any predictor \\(\\beta_j\\) while holding all other predictors constant \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\\]\n\nvariable selection (78)\nthe task of refining a model to include only the variables associated with the response\n\nnull model (79)\na model that conatins an intercept, but no predictors; used as a first stage in forward selection\n\nforward selection (79)\na variable selection method that starts with a null model, and then runs simple linear regressions on all predictors \\(p\\) and adding the one that results in the lowest RSS; repeated until some threshold is reached\n\nbackwards selection (79)\na variable selection method that starts with a model with all predictors, and removing the one with the lowest \\(p\\)-value until all remaining predictors are significant, whether by \\(p\\)-value or some other criterion\n\nmixed selection (79)\na hybrid approach starting with a null model, adding predictors one at a time that produce the best fit, and removing any that acquire a larger \\(p\\)-value in the process until all predictors are added or eliminated\n\ninteraction (81)\nwhen predictors affect each other, in addition to providing their own effect on the model\n\nconfidence interval (82)\na range with a percentage component in which there is that percentage chance that the true value of an estimated parameter lies; a 95% confidence interval is a range in which we can be 95% certain \\(f(X)\\) will be found\n\nprediction interval (82)\nsimilar to confidence interval, but a prediction range within which we are \\(X\\)% certain that any singular future observation will fall, rather than a statistic like an overall mean; a 95% prediction interval is a range in which we are confident that 95% of future observations will fall. Prediction ranges are substantially wider than confidence intervals.\n\nqualitative predictor / factor (83)\na categorical predictor with a fixed number of factors, like “yes” / “no” or “red” / “yellow” / “green”\n\ndummy variable (83)\na numeric representation of a factor to use in a model, as in representing “yes” / “no” factor variables as 1 / 0 in a regression\n\nbaseline (86)\nthe factor level where there is no dummy variable; a factor with 3 levels will use 2 dummy variables, with the factor’s absence signifying the 3rd value (usually 0)\n\nadditivity assumption (87)\nthe assumption that the association between a predictor \\(X\\) and the response \\(Y\\) does not depend on the value of other predictors; used by the standard linear regression model\n\nlinearity assumption (87)\nthe assumption, also used by the standard linear regression model, that unit changes in \\(X_j\\) result in the same change to Y regardless of its value\n\ninteraction term (88)\nthe product of two predictors in a multiple regression model, quantifying their effect on each other\n\nmain effect (89)\nisolated effects; the effect of a single predictor on the outcome\n\nhierarchical principle (89)\nthe principle that main effects should be left in a model even if they are statistically insignificant, if they are also part of an interaction that is significant\n\npolynomial regression (91)\nan extension of linear regression to accommodate non-linear relationships\n\nresidual plot (93)\na plot of the residuals or errors (\\(e_i = y_i - \\hat{y}_i\\)), used to check for non-linearity (a potential problem that would likely indicate something was missed in the model)\n\ntime series (94)\ndata consisting of observation made at discrete points in time\n\ntracking (95)\nwhen (residuals / variables?) tend to have similar values\n\nheteroscedasticity (96)\nnon-constant variances in errors; “unequal scatter”\n\nhomoscedasticity (extra)\nconstant variances in errors; follows the assumption of equal variance required by most methods\n\nweighted least squares (97)\nan extension to ordinary least squares used in circumstances of heteroscedasticity, to weight data points proportionally with the inverse variances\n\noutlier (97)\nan observation whose value is very far from its predicted value\n\nstudentized residual (98)\na residual divided by its estimated standard error; observations with student residuals higher than 3 (indicating 3 standard deviations) are likely outliers\n\nhigh leverage (98)\nobservations with an unusual \\(x_i\\) value, far from other / expected \\(x\\) values\n\nleverage statistic (99)\na quantification of a point’s leverage \\[h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^{n}(x_{i`} - \\bar{x})^2}\\]\n\ncollinearity (99)\nwhen two or more predictor variables are closely related to each other\n\npower (101)\nthe probability of a test correctly detecting a nonzero coefficient (and correctly rejecting \\(H_0 : \\beta_j = 0\\))\n\nmulticollinearity (102)\nwhen collinearity exists between three or more predictors even when no pair of predictors is collinear (or correlated)\n\nvariance inflation factor (102)\n“the ratio of the variance of \\(\\hat{\\beta_j}\\) when fitting the full model divided by the variance of \\(\\hat{\\beta_j}\\) on its own”; smallest possible value of 1 indicates the absence of collinearity, 5-10 indicates a “problematic amount”. \\[\\text{VIF}(\\hat{\\beta_j}) = \\frac{1}{1 - {R_{X_j|X_-j}^2}}\\] “\\(R_{X_j|X_-j}^2\\) is the \\(R^2\\) from a regression of \\(X_j\\) onto all of the other predictors”\n\nK-nearest neighbors regression (105)\na mode of regression that seeks to classify observations by their proximity to classified neighbors\n\ncurse of dimensionality (107)\nwhen an observation has no nearby neighbors due to a high number of dimensions exponentially increasing the available space for other observations to be spread out in\n\nChapter 4\nqualitative / categorical (129)\ninterchangeable term for a variable with a non-quantitative value, such as color\n\nclassification (129)\npredicting a qualitative response for an observation\n\nclassifier (129)\na classification technique, such as: logistic regression, linear discriminant analysis, quadratic discriminant analysis, naive Bayes, and K-nearest neighbors\n\nlogistic function (134)\na specific function returning an S-shaped curve with values between 0 and 1, used in logistic regression \\[p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}}\\]\n\nodds (134)\nthe likelihood of a particular outcome: the ratio of the number of results that produce the outcome versus the number that do not, between 0 and \\(\\infty\\) (very low or very high probabilities) \\[odds = \\frac{p(X)}{1 - p(X)} = e^{\\beta_0 + \\beta_1X}\\] Note: odds is not the same as probability! Odds of 1/4 means a 20% probability, not 25%.\n\nTo convert from odds to probability: divide odds by (1 + odds), as in:\n\\[\\frac{1}{4} \\div \\left[ 1 + \\frac{1}{4} \\right] = \\frac{1}{4} \\div \\frac{5}{4} = 1/5 = 0.2\\]\nTo convert from probability to odds, divide probability by (1 - probability), as in: \\[\\frac{1}{5} \\div \\left[ 1 - \\frac{1}{5} \\right] = \\frac{1}{5} \\div \\frac{4}{5} = \\frac{1}{5} \\times \\frac{5}{4} = 1/4 = 0.25\\] log odds / logit (135) : the log of the odds \\[\\text{log} \\left( \\frac{p(X)}{1 - p(X)} \\right)\\]\nlikelihood function (135)\na function that produces the closest possible match of a set of observations, for use in predicting the classifications of other points hairy equation not rendered\n\nconfounding (139)\nwhen predictors correlate; usually this is something to work to avoid\n\nmultinomial logistic regression (140)\nlogistic regression into more than 2 categories\n\nsoftmax coding (141)\nan alternative coding for multiple logistic regression that treats all classes symmetrically, rather than establishing one as a baseline\n\n(probability) density function (142)\na function returning the likelihood of a particular value occurring in a given sample, between 0 and 1; often used in pairs to establish likelihood of a value within a range rather than the likelihood of an infinitely-thin single “slice”\n\nprior (142)\nthe probability that a random observation comes from class \\(k\\)\n\nposterior (142)\nthe probability that an observation belongs to a class \\(k\\) given its predictor value\n\nnormal / Gaussian (143)\ncharacterized by a bell-shaped curve, not uniformly distributed but tending towards a likeliest/central value\n\noverfitting (148)\nmapping too closely to idiosyncrasies in training data, increasing variance in a model\n\nnull classifier (148)\na classifier that always predicts a zero or null status\n\nconfusion matrix (148)\na matrix showing how many predictions were made, and how accurate the classifications were (predicted x actual, hits on TL-BR diagonal and misses on BL-TR diagonal)\n\nsensitivity (149)\nthe percentage of a class (like defaulters) that is correctly identified\n\nspecificity (149)\nthe percentage of a different class (like non-defaulters) that is correctly identified\n\nNote: sensitivity and specificity can and should be separately considered when evaluating ML methods. Each column (class) of a confusion matrix has a sensitivity and a specificity. In a 2x2, this is simple; in larger than 2x2, it involves summation.\n\\[\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\\] True positives is the cell in the class column containing the number of correct predictions in a category; the false negatives are the rest of the column.\n\\[\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives + False Positives}}\\] True negatives is the sum of cells, in the other columns, that correctly do not predict the class (even if they wrongly predict some other class as well); the false positives are the rest of the row for the class that incorrectly predicted that class.\nROC charts show the true positive rate (sensitivity) on the Y axis, and the false positive rate (1 - sensitivity) on the X axis.\nROC curve (150)\nReceiver Operating Characteristics; name of a curve showing the overall performance of a classifier resembling the top left corner of a rounded rectangle; area under the curve (AUC) indicates the percentage of correct classifications; the closer it is to square, the bigger the AUC, the better the classifier. The Y axis is true positive rate (sensitivity). the X axis is false positive rate (1 - sensitivity).\n\nStatQuest: AUC (area under the curve) of the ROC shows the overall effectiveness / quality of a model; good models will fill the space more. And the points of the individual curve help indicate the best thresholds to pick for the classifier; points with X = 0 have no false positives, and points with Y = 1 get all of the true positives.\nAlso, precision is another option for the X-axis, rather than if it’s more important to know the proportion of the true positives were correctly identified, as opposed to the false positive rate.\n\\[\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\\]\nmarginal distribution (155)\nthe distribution of an individual predictor\n\njoint distribution (155)\nthe association between different predictors\n\nkernel density estimator (156)\n“essentially a smoothed version of a histogram”\n\nChapter 5\nmodel assessment (197)\nthe process of evaluating a model’s performance\n\nmodel selection (197)\nthe process of selecting the proper level of flexibility for a model\n\nvalidation set approach (198)\nrandomly dividing a set of observations into a training set and a validation or hold-out set, to assess the test error rate\n\nleave one out cross-validation (LOOCV) (200)\nusing a single observation for a validation set and the rest as a training set\n\nk-fold cross-validation (CV) (203)\ndividing a set of observation into \\(k\\) groups of approximately equal size, using the first as a validation set and fitting the method on the remaining \\(k-1\\) folds\n\nbootstrap (209)\na tool to quantify the uncertainty associated with a given estimator or statistical learning method\n\nsampling with replacement (211)\npicking from a set without removing the picked elements\n\n\nhttp://faculty.ucr.edu/~hanneman/nettext/C1_Social_Network_Data.html↩︎\n",
    "preview": {},
    "last_modified": "2022-03-09T17:19:42-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ps753-glossary/",
    "title": "PS753 Glossary",
    "description": "Glossary for PS753 coursework",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-01-29",
    "categories": [
      "social networks",
      "glossary"
    ],
    "contents": "\n\nContents\nWeek 1 - Introduction\nClass lecture 1a: Nodes, Edges and Network Samples\nClass lecture 1b: Ties and Adjacency Matrices\nClass lecture 1c: Edgelists\nHanneman, Robert A. and Riddle, Mark (2005), chapter 1 (Social network data):\nLazer (2011)\nBorgatti et al. (2009)\n\nWeek 2 - Network Structure\nClass lecture 2a: Network Structure - Walks, Paths, and Distance\nClass lecture 2b: Graph Substructures and Components\nClass lecture 2c: Dyad and Triad Census\nClass lecture 2d: Transitivity and Clustering Coefficient\n\nWeek 3 - Network Degree\nClass lecture 3a: Degree\nClass lecture 3b: Centrality vs. Centralization\nClass lecture 3c: Network Density\n\nWeek 4 - Status & Eigenvectors\nClass lecture 4a: Status and Hierarchy\nClass lecture 4b: Status and Prestige\nClass lecture 4c: Hubs and Bridges\nClass lecture 4d: Status and Core/Periphery Networks\n\n\n\n > Academics > PS 753 > Glossary\n\n\nA social network is a set of actors (or points, or nodes, or agents) that may have relationships (or edges, or ties) with one another. Networks can have few or many actors, and one or more kinds of relations between pairs of actors.\n– (Hanneman, Robert A. and Riddle, Mark 2005, chap. 2)\n\nWeek 1 - Introduction\nClass lecture 1a: Nodes, Edges and Network Samples\n(Note: some terms overlap somewhat or have context-dependent synonyms.)\nnode / vertex\na junction in a network where two or more lines (edges) intersect; a dot connecting lines.\n\ntie / edge / link / relation\nlines connecting nodes, which indicate some sort of connection or relationship.\n\nnode / ego / actor\n\\(i\\) - the node being discussed or focused on; also note “Actors are described by their relations, not by their attributes.”1\n\nalter\n\\(j\\) - the node that \\(i\\) connects to\n\nnetwork population\n\\(n\\) - the size of the population or count of nodes\n\nWith set notation, we define \\(i\\) as a set of \\(n\\) elements: \\[i \\in 1, 2, 3 \\dots n\\]\nand \\(j\\) similarly as a set of \\(n\\) elements, except that \\(j\\) cannot equal \\(i\\) (a node cannot connect to itself): \\[j \\in 1, 2, 3 \\dots n, i \\ne j\\]\ninteraction threshold\na measure to determine whether two entities have a sufficient connection to be considered to have a link between them\n\nsnowball sample\nan entity group formed by starting with “a focal actor or set of actors”2 and “rolling outwards” to its connections until all nodes or actors (in a limited set) are located, or a decision to stop is made\n\negocentric name generator\na mode of building an entity group (an egocentric network) defined by connection to a single central node; like a snowball sample that doesn’t expand past the first set of connections\n\nClass lecture 1b: Ties and Adjacency Matrices\nadjacency matrix\nan \\(n \\times n\\) matrix depicting connections between \\(n\\) nodes as 0 or 1, where 1 is a connection (or vertex/edge) and 0 is the absence of one:\n\n\nA\nB\nC\nD\nA\n-\n0\n1\n0\nB\n0\n-\n1\n0\nC\n1\n1\n-\n0\nD\n0\n0\n0\n-\ndirected tie\na relationship where \\(W_{i,j} \\ne W_{j,i}\\), as might be used to represent a transfer of resources from one node to another; graphed with arrows\n\nsymmetric tie\na relationship where \\(W_{i,j} = W_{j,i}\\), with no direction; graphed with lines\n\nbinary tie\na tie where \\(W_{i,j}\\) is 0 or 1, as in the example above, indicating the absence or presence of a tie (also dichotomous)\n\nvalued tie\na tie where \\(W_{i,j}\\) is a value \\(v\\), indicating a weight or magnitude of the connection; may be graphed with line attributes such as weight, color, etc\n\nTies are symmetric or directed, and binary or valued.\nClass lecture 1c: Edgelists\nedgelist\na table indicating edges in a network, with at least “from” and “to” columns, and possibly additional columns for attributes or values\n\nFrom\nTo\nValue\nA\nB\n5\nA\nE\n2\nB\nA\n-1\nB\nC\n4\nB\nD\n2\nC\nA\n-4\nEdgelists are more efficient in sparse networks, as they only list actual connections rather than being a matrix of all possible connections\nHanneman, Robert A. and Riddle, Mark (2005), chapter 1 (Social network data):\nbinary measures of relations\nundirected relations, 0 or 1 for the absence or presence of a connection\n\nmultiple-category nominal measures of relations\ndirected relations with categories (like “friend, lover, business relationship, kin, or no relationship”)\n\ngrouped ordinal measures of relations\nmeasures that reflect a level of intensity or degree; often turned into binary measures by means of a threshold or cut-off\n\nfull-rank ordinal measures of relations\nan ordering from 1 to \\(n\\) of an actor’s relations (uncommon in social networks)\n\ninterval measures of relations\ncontinuous measures that express the strength of connections by comparison with others, to be able to say “this tie is twice as strong as that tie”; the “most advanced” level of measurement\n\nLazer (2011)\nhomophily\nthe idea that individuals who are similar to one another are more likely to form ties\n\nwhole network data\nrelational information about a whole set (or subset) of data, with all of the actors’ relations to each other considered\n\negocentric data\nrelational information about a set of nodes connected to one particular node and not to each other\n\ndiameter\nthe maximum degree of separation between any two nodes in the network\n\none-mode data\nties among one set (or category) of agent, such as nations in the context of trade\n\ntwo-mode data / bipartite data\nties between different sets (or categories) of agents, such as ties between nations and international organizations; a network split into two parts, each of whose nodes only connect to nodes in the opposite part, not to nodes within its own part; “affiliation network” in Wasserman and Faust (1994)\n\nBorgatti et al. (2009)\nsociometry\n“a technique for eliciting and graphically representing individuals’ subjective feelings toward one another”\n\nstrength of weak ties (SWT) theory\nthe theory that one is likelier to hear new information from people they aren’t closely connected to in a network (c.f. homiphily)\n\ncentrality\na family of positional properties of nodes in a network\n\nFreeman’s betweenness\na type of centrality where a node is frequently along the shortest path between pairs of nodes, giving control over flow or power\n\nopportunity-based antecedents\n“the likelihood that two nodes will come into contact” - when considering the formation of ties\n\nbenefit-based antecedents\n“some kind of utility maximization or discomfort minimization that leads to tie formation”\n\nnode homogeneity\na category of node outcomes referring to the similarity of nodes\n\nperformance\na category of node outcomes referring to some good (such as strong performance)\n\nWeek 2 - Network Structure\nClass lecture 2a: Network Structure - Walks, Paths, and Distance\nConnections between nodes:\nadjacent\na direct connection between nodes; not necessarily bilateral in directed networks. A leading to B (“A adj B”) does not mean that B leads to A (\"B adj A)\n\nreachable\nwhether a node is reachable from another node, regardless of distance\n\ndistance\nthe number of ties (steps, edges) that must be traversed to reach a target node\n\nwalk\nsequence (not path, see below) that connects two nodes, consisting of the nodes and edges\n\ntrail\na walk that can only go through each edge / tie once, but can hit the same node more than once\n\npath\na trail that only hits each node and edge once; the start and end node may be the same\n\ngeodesic distance\nthe shortest path between two nodes; by definition, not a trail or walk because repeated segments wouldn’t be the shortest; with binary data, the number of edges between the nodes; with weighted data, might be a sum or some other calculation of “effort”\n\nClass lecture 2b: Graph Substructures and Components\nNetwork substructures\ndyad, triad, clique\ntwo, three, or four-or-more connected nodes\n\ncomplete graph\na network where every node is directly connected to every other node\n\nconnected graph\na network where every node is indirectly connected to every other node\n\nunconnected graph\na network where at least one node is unreachable\n\ncomponent\nthe set of all points that constitutes a connected subgraph within a network\n\nmain component\nthe largest component within a network\n\nminor component\na smaller one, possibly one of many\n\npendant\na node with only one link or edge to a network, “dangling”\n\nisolate\nan unconnected node\n\nClass lecture 2c: Dyad and Triad Census\nmutual dyad\na dyad where both nodes connect to each other, as in an undirected network\n\nasymmetric dyad\na dyad (in a directed network) where one node connects to another, but non-reciprocally (one way only)\n\nnull\na dyad of two unconnected nodes\n\n(empty / one edge / two path / triangle) triad\na triad with zero, one, two or three edges between three nodes (all four possible permutations)\n\nbalance theory\nthe theory that two nodes connected to a common node will also develop connections to each other\n\nglobal transitivity index\nthe proportion of triads in a network that are complete (with 3 connections between them)\n\nThere is a vocabulary for triads in directed networks describing the 16 possible permutations of ties (or the absence thereof) among 3 nodes, counting the number of mutual, asymmetric and null dyads, with direction indicators, like 003 or 120D - see the slide at 3:45\nvacuously transitive triad\na triad where (to be continued…) (5 of 16 possibilities)\n\nintransitive triad\na triad where (to be continued…) (7 of 16 possibilities)\n\ntransitive triad\na triad where (to be continued…) (4 of 16 possibilities)\n\n(See Alhazmi, Gokhale, and Doran (2015))\nClass lecture 2d: Transitivity and Clustering Coefficient\nlocal transitivity / local clustering coefficient\nthe likelihood that the neighbors of a node are also connected to each other; the number of connections that do exist over the number of connections that could exist\n\n In the example above, there are 8 nodes that Homer (center) could connect to; among those 8 nodes, there are \\(7 + 6 + 5 + 4 + 3 + 2 + 1 = 28\\) possible undirected connections (not connecting to Homer), and 9 of those 28 do exist, for a local clustering coefficient of \\(9/28 \\approx 0.32\\).\naverage clustering coefficient\nthe average of the local clustering coefficient of all nodes in the network\n\nWeek 3 - Network Degree\nClass lecture 3a: Degree\n(vertex) degree\nthe number of links that a node has; the number of nodes it’s connected to\n\ndegree distribution\na distribution showing the number of nodes of a network that have each degree level\n\nindegree\nthe number of links that a node receives in a directed network\n\noutdegree\nthe number of links that a node sends in a directed network\n\nClass lecture 3b: Centrality vs. Centralization\ncentrality\na measure of the prominence of one node relative to others; can be variously defined\n\n(degree) centrality\nproportional the the number of other nodes to which a node is linked\n\ncentralization\na property of a graph or network, referring to its overall cohesion; comparing most central point to all other points; ratio of the actual sum of differences to the maximum possible sum of differences\n\nClass lecture 3c: Network Density\nnetwork density\nnumber of ties as a proportion of the maximum possible number of ties; varies from 0 to 1, calculation will vary by whether network is undirected or directed (twice as many potential connections)\n\nWeek 4 - Status & Eigenvectors\nClass lecture 4a: Status and Hierarchy\ncloseness centrality\nthe sum of geodesic distances (shortest paths) to all other points in the graph. Divide by \\(n-1\\), then invert. A measure of how close a node is to all of the other nodes in the network.\n\nClass lecture 4b: Status and Prestige\nprestige\nsignal of the quality of a node, of a node’s visibility within the network\n\neigenvector centrality\n“takes into account the centrality of other nodes to which a node is connected. That is, being connected to other central nodes increases centrality.” Takes into account the centrality of the nodes a node is connected to, considering the importance of the connected nodes and not just their quantity or path length. A node with high eigenvector centrality is connected to significant numbers of other highly central nodes. \\[\\lambda \\text{c} = \\text{Wc}\\]\n\nBonacich Power\n“A closely related concept, but includes a weighting factor that emphasizes global vs. local structure (and negative connections)” In contrast to eigenvector centrality, penalizes nodes connected to other well-connected nodes, under the theory that being connected to strong or powerful nodes means a node has less influence and power than it would if it were connected to fewer or weaker nodes. \\[\\text{c}(\\alpha, \\beta) = \\alpha(\\text{I} - \\beta\\text{W})^{-1}\\text{W1}\\]\n\nClass lecture 4c: Hubs and Bridges\nbridge\na node with few ties to central actors\n\nhub\na node with many ties to peripheral actors\n\nreflected centrality\nthe degree of centrality that a node receives from a connected node that is due to that node’s centrality. If A has influential friend B, then A’s eigenvector centrality will be higher; reflected centrality is the portion of A’s centrality that comes from B’s centrality.\n\nderived centrality\nthe remainder of the eigenvector centrality that A receives from B in the example above, that is not due to B’s centrality but just from B being a connected node\n\nReflected and derived centrality can be represented in the following matrix by Mizruchi et al:\n\nHigh reflected centrality\nLow reflected centrality\nHigh derived centrality\nCosmopolitans\nPure bridges\nLow derived centrality\nPure hubs\nPeripherals\nThe four prototypes are:\ncosmopolitans\nwell-connected nodes that are connected to other well-connected nodes; the “cool kids” table\n\npure hubs\nwell-connected nodes that are connected to nodes that are not well-connected; the cool kid who talks to uncool kids\n\npure bridge\na node connected to few but high-centrality nodes; a friend of the cool kid with few other friends\n\nperipheral\na node with few and low-centrality connections; me and my D&D nerd friends in high school\n\nClass lecture 4d: Status and Core/Periphery Networks\nbetweenness centrality\na count of the number of shortest paths between nodes that pass through another node\n\n\n\n\nAlhazmi, Huda, Swapna S. Gokhale, and Derek Doran. 2015. “Understanding Social Effects in Online Networks.” In 2015 International Conference on Computing, Networking and Communications (ICNC), 863–68. Garden Grove, CA, USA: IEEE. https://doi.org/10.1109/ICCNC.2015.7069459.\n\n\nBorgatti, Stephen P., Ajay Mehra, Daniel J. Brass, and Giuseppe Labianca. 2009. “Network Analysis in the Social Sciences.” Science, February. https://doi.org/10.1126/science.1165821.\n\n\nHanneman, Robert A., and Riddle, Mark. 2005. “Introduction to Social Network Methods.” Introduction to Social Network Methods. http://faculty.ucr.edu/~hanneman/nettext/.\n\n\nLazer, David. 2011. “Networks in Political Science: Back to the Future.” PS: Political Science & Politics 44 (1): 61–68. https://doi.org/10.1017/S1049096510001873.\n\n\nWasserman, Stanley, and Katherine Faust. 1994. Social Network Analysis: Methods and Applications. Structural Analysis in the Social Sciences. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511815478.\n\n\nHanneman, Robert A. and Riddle, Mark (2005)↩︎\nHanneman, Robert A. and Riddle, Mark (2005)↩︎\n",
    "preview": {},
    "last_modified": "2022-03-15T14:38:02-04:00",
    "input_file": {}
  }
]
