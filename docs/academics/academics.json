[
  {
    "path": "academics/ps753-useful-r-tidbits/",
    "title": "Useful R tidbits: Social Networks",
    "description": "Tasty R snacks that do useful things in Social Networks",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-08",
    "categories": [],
    "contents": "\n\n > Academics > PS753 > Useful R tidbits\n\nThis document contains R snippets pertaining to work in the Social Networks class. There is also one for the Machine Learning class.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-08T17:28:54-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ps753/",
    "title": "PS753",
    "description": "Political and Social Networks, Spring 2022.",
    "author": [],
    "date": "2022-02-08",
    "categories": [],
    "contents": "\n\n > Academics > PS 753\n\nWork and assignments from this class will be added here as we go. (This part has to be done by hand in Distill, unfortunately.)\nMy ongoing glossary page; feel free to comment, or make additions or corrections via comments or github.\nSome useful R tidbits from the course material and related sources.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-08T17:25:09-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ms797/",
    "title": "MS797",
    "description": "Machine Learning for Social Sciences, Spring 2022.",
    "author": [],
    "date": "2022-02-07",
    "categories": [],
    "contents": "\n\n > Academics > MS797\n\nWork and assignments from this class will be added here as we go. (This part has to be done by hand in Distill, unfortunately.)\nMy ongoing glossary page; feel free to comment, or make additions or corrections via comments or github.\nSome useful R tidbits from, or inspired by, the text.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-07T07:48:42-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ms797-useful-r-tidbits/",
    "title": "Useful R tidbits: Machine Learning",
    "description": "Tasty R snacks that do useful things in Machine Learning",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-02-06",
    "categories": [],
    "contents": "\n\nContents\nChapter 3: Linear Regression\nnames(): see the attributes of a model\ncoef(): see the model’s coefficients\nconfint(): see confidence intervals for a model\npredict(): see confidence OR prediction intervals for a model\narranging plots in grids\n\nMultiple Linear Regression\nInteraction Terms\nNon-linear Transformations of the Predictors\nQualitative Predictors\n\n\n > Academics > MS797 > Useful R tidbits\n\nThis document contains R snippets pertaining to work in the Machine Learning class. There is also one for the Social Networks class.\nMost of this comes directly from the resources from the 2nd edition of ISLR, either direct quotes or things that build on them. The Lab section of each chapter contains useful walkthroughs and code illustrating the chapter’s key concepts, and there’s always something to learn about R and how different people use it.\n\nEverything here is, unless otherwise marked, either the work of, or derivative of the work of, the original authors of the book.\nChapter 3: Linear Regression\n\n\n\nnames(): see the attributes of a model\nnames() is a nice, simple way to see the attributes of a linear model:\n\n\nattach(Boston)\nlm.fit <- lm(medv ~ lstat)\nnames(lm.fit)\n\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(): see the model’s coefficients\ncoef() is a convenient way to get the coefficients attribute:\n\n\ncoef(lm.fit)\n\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nNote: this is equivalent to lm.fit$coefficients, but slightly easier on the eyes.\nconfint(): see confidence intervals for a model\n\n\nconfint(lm.fit)\n\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\nNote: levels can be added with the level parameter (the default is 0.95):\n\n\nconfint(lm.fit, level = 0.90)\n\n\n                  5 %       95 %\n(Intercept) 33.626697 35.4809847\nlstat       -1.013877 -0.8862212\n\npredict(): see confidence OR prediction intervals for a model\n\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\n\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\nNote: prediction intervals are defined on p. 82; in brief, it is the interval in which we are \\(X\\)% certain that any future observation will fall.\nNote: the data.frame() call in the above parameter lists produce a nifty little data frame that is used by predict to identify the column to do calculations on, and the desired levels:\n\n\ndata.frame(lstat = (c(5, 10, 15)))\n\n\n  lstat\n1     5\n2    10\n3    15\n\nNote: the book uses the base R graphics:\n\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\nThe tidyverse/ggplot version of this is:\n\n\nlibrary(tidyverse)\nggplot(Boston, mapping = aes(x = lstat, y = medv)) +\n  geom_point() +\n  geom_abline(intercept = coef(lm.fit)[1],\n              slope = coef(lm.fit)[2])\n\n\n\n\n…a bit more work, but more modern.\narranging plots in grids\nThe book uses base R, again, with par() and mfrow() to arrange plots:\n\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\nggplot uses facet() to arrange plots from the same data set in grids.\nPlots from different data sets need additional packages to be combined; one option is the cowplot library and the plot_grid() function.\nXXX\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\n\nplot(hatvalues(lm.fit))\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n\n375 \n375 \n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\nMultiple Linear Regression\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax {} is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\n\nlm.fit <- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 33.22276    0.73085  45.458  < 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  < 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: < 2.2e-16\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\n\nlm.fit <- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  < 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: < 2.2e-16\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\n\nlibrary(car)\nvif(lm.fit)\n\n\n    crim       zn    indus     chas      nox       rm      age \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 \n     dis      rad      tax  ptratio    lstat \n3.954037 7.445301 9.002158 1.797060 2.870777 \n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\n\nlm.fit1 <- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  < 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: < 2.2e-16\n\nAlternatively, the update() function can be used.\n\n\nlm.fit1 <- update(lm.fit, ~ . - age)\n\n\n\nInteraction Terms\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:black tells R to include an interaction term between lstat and black. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  < 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: < 2.2e-16\n\nNon-linear Transformations of the Predictors\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\n\nlm.fit2 <- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 42.862007   0.872084   49.15   <2e-16 ***\nlstat       -2.332821   0.123803  -18.84   <2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\n\nlm.fit <- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(>F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\n\nlm.fit5 <- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       22.5328     0.2318  97.197  < 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  < 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  < 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: < 2.2e-16\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -76.488      5.028  -15.21   <2e-16 ***\nlog(rm)       54.055      2.739   19.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: < 2.2e-16\n\nQualitative Predictors\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\n\nhead(Carseats)\n\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age\n1  9.50       138     73          11        276   120       Bad  42\n2 11.22       111     48          16        260    83      Good  65\n3 10.06       113     35          10        269    80    Medium  59\n4  7.40       117    100           4        466    97    Medium  55\n5  4.15       141     64           3        340   128       Bad  38\n6 10.81       124    113          13        501    72       Bad  78\n  Education Urban  US\n1        17   Yes Yes\n2        10   Yes Yes\n3        12   Yes Yes\n4        14   Yes Yes\n5        13   Yes  No\n6        16    No Yes\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\n\nlm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  < 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  < 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  < 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  < 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: < 2.2e-16\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n\n",
    "preview": "academics/ms797-useful-r-tidbits/ISLR-cover.png",
    "last_modified": "2022-02-08T17:29:03-05:00",
    "input_file": {},
    "preview_width": 750,
    "preview_height": 1263
  },
  {
    "path": "academics/ms797-glossary/",
    "title": "MS797 Glossary",
    "description": "Glossary for MS797 coursework",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-01-30",
    "categories": [],
    "contents": "\n\nContents\nChapter 2\nChapter 3\n\n\n > Academics > MS797 > Glossary\n\nPage numbers in parenthesis after terms, from ISLR 2nd edition. Non-page numbers indicate other sources; “biostats” references material from Biostatistics 690Z (Health Data Science: Statistical Modeling), fall 2021.\nChapter 2\ninput variable (15)\nalso predictor, independent variable, feature; usually written \\(X_1, X_2\\), etc. The parameter or parameters we are testing to see if they are related to or affect the output.\n\noutput variable (15)\nalso response, dependent variable; usually written \\(Y\\). The outcome being measured.\n\nerror term (16)\n\\(\\epsilon\\) in the equation\\[Y = f(X) + \\epsilon\\] a random quantity of inaccuracy, independent of X and with mean 0.\n\nsystematic (16)\n\\(f\\) in the equation\\[Y = f(X) + \\epsilon\\] the function that describes the (systematic) information \\(X\\) provides about \\(Y\\). This plus the error term equals \\(Y\\).\n\nreducible error (18)\nThe amount of the error \\(\\epsilon\\) that could be eliminated by improving our estimator \\(\\hat{f}\\); the difference between \\(\\hat{f}\\) and \\(f\\). This book and course is mostly about ways to minimize the reducible error.\n\nirreducible error (18)\nThe amount of \\(\\epsilon\\) that could not be reduced even if \\(f\\) was a perfect estimator of \\(Y\\). Always greater than 0. Could be due to hidden variables in \\(\\epsilon\\), or random fluctuations in Y, like a measure of “[a] patient’s general feeling of well-being on that day”.\n\nexpected value (19)\naverage value of an expected measure.\n\ntraining data (21)\ndata used to develop the model for estimating \\(f\\).\n\nparametric methods (21)\nA model based on one or more input parameters, that yields a value for Y, as in: \\[f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\] \\[Y \\approx \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\\] \\[\\text{income} \\approx \\beta_0 + \\beta_1 \\times \\text{education} + \\beta_2 \\times \\text{seniority}\\] This creates a predictive, inflexible model which usually does not match the true \\(f\\), but which has advantages of simplicity and interpretability. It can be used to predict values for \\(Y\\) based on its parameters, or inputs. Linear and logistic regression are parametric.\n\nnon-parametric methods (23)\nmethods that do not attempt to estimate \\(f\\). More flexible and have the potential to very closely match observations, but with the risk of overfitting the data and increasing the variance of subsequent observations. They require much more data than parametric models, and may be difficult to interpret, K-Nearest Neighbor and Support Vector Machines are non-parametric.\n\nprediction (26)\nseeking to guess the value of an response variable \\(y_i\\) given a set of observations and a predictor \\(f\\).\n\ninference (26)\na model that seems to better understand the relationship between the response and the predictors.\n\nsupervised learning (26)\na category of model that allows us to guess a \\(y_i\\) response to a set of predictor measurements \\(x_i, i = 1, \\dots, n\\).\n\nunsupervised learning (26)\na category of model in which there are observations/measurements \\(x_i, i = 1, \\dots, n\\), but no associated response \\(y_i\\). Linear regression cannot be used because there is no response variable to predict.\n\ncluster analysis (27)\nin unsupervised learning, a statistical method for determining whether a set of observations can be divided into “relatively distinct groups,” looking for similarities within the groups. (Topic modeling may be an example of this.)\n\nquantitative variables (28)\nnumeric values; age, height, weight, quantity. Usually the response variable type for regression problems.\n\nqualitative variables (28)\nalso categorical: values from a discrete set. Eye color, name, yes/no. Usually the response variable type for classification problems.\n\nregression problems (28)\nproblems with quantitative response variables. Given predictors foo, bar, and baz, how big is the frob?\n\nclassification problems (28)\nproblems with qualitative response variables. Given predictors foo, bar, and baz, is the outcome likely to be a frob, a frib or a freeb?\n\nmean squared error (MSE) (29)\nthe average squared error for a set of observations: \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2\\] MSE is small if the predicted responses are close to the true responses, and larger as it becomes less accurate; computed from training data, and Gareth et al. suggest it should be called training MSE.\n\nvariance (34)\n“the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set”\n\nbias (35)\n“the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model”, as in the error from the (presumed) linearity of a regression against non-linear data whose complexity it does not capture. More flexible models increase variance and decrease bias.\n\nbias-variance trade-off (36)\nThe tension in seeking the best model for the data between missing the true \\(f\\) with an overly simple (biased) model, vs. an overfitted model with too much variance from mapping too closely to test data.\n\nerror rate (37)\nIn classification, the proportion of classifications that are mistakes. \\[\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\neq \\hat{y}_i)\\] \\(I\\) is 1 if \\(y_i \\neq \\hat{y}_i\\) - if the guess for any given \\(y\\) is wrong. The error rate is the percentage of incorrect classifications. Also the training erorr rate.\n\nindicator variable (37)\n\\(I\\) in the error rate definition above; a logical variable indicating the presence or absence of a characteristic or trait (such as an accurate classification).\n\ntest error rate (37)\nlike the training error rate but applied to the test data. Uses \\(\\text{Ave}\\) instead of sum notation: \\[\\text{Ave}(I(y_0 \\neq \\hat{y}_0))\\] \\(\\hat{y}_0\\) is the predicted class label from the classifier.\n\nconditional probability (37)\nThe chance that \\(Y = j\\) given an observed \\(x_0\\), as in the Bayes classifier: \\[\\text{Pr}(Y = j|X = x_0)\\] In a two-class, yes/no classifier, we decide based on whether \\(\\text{Pr}(Y = j|X = x_0)\\) is \\(> 0.5\\), or not. Note that \\(Y\\) is the class, as in “ham”/“spam”, not a \\(y\\)-axis coordinate.\n\nBayes decision boundary (38)\na visual depiction of the line of 50% probability dividing (exactly two?) classes in a two-dimensional space\n\nBayes error rate (38)\nthe expected (average) probability of classification error over all values of X in a data set. \\[1 - E(\\underset{j}{maxPr}(Y = j|X))\\] The \\(\\underset{j}{maxPr}\\) whichever of the \\(j\\) classes has the highest probability for any given value of \\(X\\). Again, \\(Y\\) is not a y-axis coordinate of a two-dimensional space, it’s the class of the classification: “yes”/“no”, “ham”/“spam”, “infected”/“not infected”. Also: “The Bayes error rate is analogous to the irreducible error, discussed earlier.”\n\nK-nearest-neighbors (KNN) (39)\na classifier that assigns a class Y to an observation based on the population proportions of its nearest neighbors; a circular “neighborhood” on a two-dimensional plot. It looks at actual data points that have been classified, and asks what any given non-classified point would be classified as based on its nearest neighbors.\n\nChapter 3\nSynergy effect / interaction effect (60)\nwhen two or more predictors affect each other as well as the outcome; when 50k each in TV or radio ads give different results than 100k in either one\n\nSimple linear regression\nthe simplest model, predicting \\(Y\\) from a single predictor \\(X\\). \\[Y \\approx \\beta_0 + \\beta_1X\\] \\(\\approx\\) = “is approximately modeled as”\n\nleast squares (61)\nthe most common measure of closeness of a regression line to its data points, the sum of squares of the distances between the points and the closest point on the line (directly above or below)\n\nresidual (61)\nthe difference between \\(y_i\\) and \\(\\hat{y}_i\\), also \\(e_i\\); the difference between the \\(i\\)th response variable and the \\(i\\)th response variable predicted by the model\n\nresidual sum of squares (RSS) (62)\nthe sum of the squared residuals for each point on the regression line \\[\\text{RSS} = e_1^2 + e_2^2 + \\dots + e_n^2\\] Formulas for \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) are on p. 62\n\nintercept (\\(\\beta_0\\)) (63)\nthe expected value of \\(Y\\) when \\(X = 0\\)\n\nslope (\\(\\beta_1\\)) (63)\nthe average increase in \\(Y\\) associated with a one-unit increase in \\(X\\)\n\nerror term (\\(\\epsilon\\)) (63)\nwhatever we missed with the model, due to the true model not being linear (it almost never is), measurement error, or other variables that cause variation in \\(Y\\)\n\npopulation regression line (63)\n“the best linear approximation to the true relationship between \\(X\\) and \\(Y\\)” \\[Y = \\beta_0 + \\beta_1X + \\epsilon\\] least squares line (63)\n\nthe regression line made of the least-squares estimates for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nbias (65)\nin an estimator, something that systematically misses the true parameter; for an unbiased estimator, \\(\\hat{\\mu} = \\mu\\) when averaged over (huge) numbers of observations\n\nstandard error (SE) (65)\nthe average amount that an estimate \\(\\hat{\\mu}\\) (sample mean) differs from the actual value of \\(\\mu\\) (population mean) \\[\\text{Var}(\\hat{\\mu}) = \\text{SE}(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n} (\\text{also} = \\frac{\\sigma}{\\sqrt{n}})\\] \\(\\sigma\\) is “the standard deviation of each of the realizations \\(y_i\\) of \\(Y\\). Since \\(\\sigma^2\\) is divided by \\(n\\), the standard error shrinks as observations increase. It represents the amount we would expect means of additional samples to”jump around\" simply due to random chance and the limitations of the model’s accuracy.1\n\nresidual standard error (RSE) (66)\nthe estimate of \\(\\sigma\\) \\[\\text{RSE} = \\sqrt{RSS / (n-2)}\\]\n\nconfidence interval (66)\na range of values within which we have a measured probability (often 95%) of containing the true value of the parameter; a 95% confidence interval in linear regression takes the form \\[\\hat{\\beta_1} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_1})\\]\n\nt-statistic (67)\nthe number of standard deviations that \\(\\hat{\\beta_1}\\) is away from \\(0\\). \\[t = \\frac{{\\hat{\\beta_1}} - 0}{\\text{SE}(\\hat{\\beta_1})}\\] For there to be a relationship between \\(X\\) and \\(Y\\), \\(\\hat{\\beta_1}\\) has to be nonzero (i.e. have a slope). The standard error (SE) of \\(\\hat{\\beta_1}\\) (in the denominator above) measures its accuracy; if it is small, then \\(t\\) will be larger, and if it is large, then \\(t\\) will be smaller. \\(t\\) is around 2 for a p-value of 0.05 (actually about 1.96, as 2 standard deviations is 95.45% of a normal distribution), and around 2.75 for a p-value of 0.01.\n\np-value (67)\nthe probability of observing a value greater than \\(|t|\\) by chance.\n\nmodel sum of squares (MSS) (biostats)\nAlso sometimes ESS, “explained sum of squares”: the total variance in the response \\(Y\\) that can be accounted for by the model \\[\\text{MSS} = \\sum(\\hat{y_i} - \\bar{y})^2\\]\n\nresidual sum of squares (RSS) (biostats)\nthe total variance in the response \\(Y\\) that cannot be accounted for by the model \\[\\text{RSS} = \\sum(y_i - \\hat{y_i})^2\\] also \\[\\text{RSS} = e_i^2 + e_2^2 + \\dots + e_n^2\\] or \\[\\text{RSS} = (y_1 - \\hat{\\beta_0} - {\\hat{\\beta_1}x_1})^2 + (y_2 - \\hat{\\beta_0} - {\\hat{\\beta_1}x_2})^ + \\dots + (y_n - \\hat{\\beta_0} - {\\hat{\\beta_1}x_n})^2\\]\n\ntotal sum of squares (TSS) (70)\nthe total variance in the response \\(Y\\); the total variability of the response about its mean \\[\\text{TSS} = \\sum(y_i - \\bar{y})^2\\] compare with RSS, the amount of variability left unexplained after the regression. TSS - RSS is the amount of variability (or error) explained by the regression (MSS).\n\nNOTE: there is a nice visual here on stackexchange; if anybody knows how to tell Zotero to use a custom bibtex citation entry over the ones it generates, please let me know so I can integrate it better here :frown:\n\\(R^2\\) statistic (70)\nthe proportion of variance in \\(Y\\) explained by \\(X\\), a range from 0 to 1 \\[R^2 = \\frac{\\text{TSS - RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\\] \\(R^2\\) values close to 1 indicate a regression that explains a lot of the variability in the response, and a stronger model. A value close to 0 indicates that the regression doesn’t explain much of the variability.\n\ncorrelation (70)\na measure of the linearity of the relationship between \\(X\\) and \\(Y\\); values close to 0 indicate weak-to-no relationship, values near 1 or -1 indicate strong positive or negative correlation (horrible formula on p.70 I don’t have time to LaTeX at the moment)\n\nstandard linear regression model (72)\nThe modal used for standard linear models, used to interpret the the effect on \\(Y\\) of a one-unit increase in any predictor \\(\\beta_j\\) while holding all other predictors constant \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p + \\epsilon\\]\n\nvariable selection (78)\nthe task of refining a model to include only the variables associated with the response\n\nnull model (79)\na model that conatins an intercept, but no predictors; used as a first stage in forward selection\n\nforward selection (79)\na variable selection method that starts with a null model, and then runs simple linear regressions on all predictors \\(p\\) and adding the one that results in the lowest RSS; repeated until some threshold is reached\n\nbackwards selection (79)\na variable selection method that starts with a model with all predictors, and removing the one with the lowest \\(p\\)-value until all remaining predictors are significant, whether by \\(p\\)-value or some other criterion\n\nmixed selection (79)\na hybrid approach starting with a null model, adding predictors one at a time that produce the best fit, and removing any that acquire a larger \\(p\\)-value in the process until all predictors are added or eliminated\n\ninteraction (81)\nwhen predictors affect each other, in addition to providing their own effect on the model\n\nconfidence interval (82)\na range with a percentage component in which there is that percentage chance that the true value of an estimated parameter lies; a 95% confidence interval is a range in which we can be 95% certain \\(f(X)\\) will be found\n\nprediction interval (82)\nsimilar to confidence interval, but a prediction range within which we are \\(X\\)% certain that any singular future observation will fall, rather than a statistic like an overall mean; a 95% prediction interval is a range in which we are confident that 95% of future observations will fall. Prediction ranges are substantially wider than confidence intervals.\n\nqualitative predictor / factor (83)\na categorical predictor with a fixed number of factors, like “yes” / “no” or “red” / “yellow” / “green”\n\ndummy variable (83)\na numeric representation of a factor to use in a model, as in representing “yes” / “no” factor variables as 1 / 0 in a regression\n\nbaseline (86)\nthe factor level where there is no dummy variable; a factor with 3 levels will use 2 dummy variables, with the factor’s absence signifying the 3rd value (usually 0)\n\nadditivity assumption (87)\nthe assumption that the association between a predictor \\(X\\) and the response \\(Y\\) does not depend on the value of other predictors; used by the standard linear regression model\n\nlinearity assumption (87)\nthe assumption, also used by the standard linear regression model, that unit changes in \\(X_j\\) result in the same change to Y regardless of its value\n\ninteraction term (88)\nthe product of two predictors in a multiple regression model, quantifying their effect on each other\n\nmain effect (89)\nisolated effects; the effect of a single predictor on the outcome\n\nhierarchical principle (89)\nthe principle that main effects should be left in a model even if they are statistically insignificant, if they are also part of an interaction that is significant\n\npolynomial regression (91)\nan extension of linear regression to accommodate non-linear relationships\n\nresidual plot (93)\na plot of the residuals or errors (\\(e_i = y_i - \\hat{y}_i\\)), used to check for non-linearity (a potential problem that would likely indicate something was missed in the model)\n\ntime series (94)\ndata consisting of observation made at discrete points in time\n\ntracking (95)\nwhen (residuals / variables?) tend to have similar values\n\nheteroscedasticity (96)\nnon-constant variances in errors; “unequal scatter”\n\nhomoscedasticity (extra)\nconstant variances in errors; follows the assumption of equal variance required by most methods\n\nweighted least squares (97)\nan extension to ordinary least squares used in circumstances of heteroscedasticity, to weight data points proportionally with the inverse variances\n\noutlier (97)\nan observation whose value is very far from its predicted value\n\nstudentized residual (98)\na residual divided by its estimated standard error; observations with student residuals higher than 3 (indicating 3 standard deviations) are likely outliers\n\nhigh leverage (98)\nobservations with an unusual \\(x_i\\) value, far from other / expected \\(x\\) values\n\nleverage statistic (99)\na quantification of a point’s leverage \\[h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^{n}(x_{i`} - \\bar{x})^2}\\]\n\ncollinearity (99)\nwhen two or more predictor variables are closely related to each other\n\npower (101)\nthe probability of a test correctly detecting a nonzero coefficient (and correctly rejecting \\(H_0 : \\beta_j = 0\\))\n\nmulticollinearity (102)\nwhen collinearity exists between three or more predictors even when no pair of predictors is collinear (or correlated)\n\nvariance inflation factor (102)\n“the ratio of the variance of \\(\\hat{\\beta_j}\\) when fitting the full model divided by the variance of \\(\\hat{\\beta_j}\\) on its own”; smallest possible value of 1 indicates the absence of collinearity, 5-10 indicates a “problematic amount”. Hairy forumla not reproduced.\n\n\nhttp://faculty.ucr.edu/~hanneman/nettext/C1_Social_Network_Data.html↩︎\n",
    "preview": {},
    "last_modified": "2022-02-07T07:48:57-05:00",
    "input_file": {}
  },
  {
    "path": "academics/ps753-glossary/",
    "title": "PS753 Glossary",
    "description": "Glossary for PS753 coursework",
    "author": [
      {
        "name": "Steve Linberg",
        "url": {}
      }
    ],
    "date": "2022-01-29",
    "categories": [],
    "contents": "\n\nContents\nWeek 1 - Introduction\nClass lecture 1a: Nodes, Edges and Network Samples\nClass lecture 1b: Ties and Adjacency Matrices\nClass lecture 1c: Edgelists\nHanneman, Robert A. and Riddle, Mark (2005), chapter 1 (Social network data):\nLazer (2011)\nBorgatti et al. (2009)\n\nWeek 2 - Network Structure\nClass lecture 2a: Network Structure - Walks, Paths, and Distance\nClass lecture 2b: Graph Substructures and Components\nClass lecture 2c: Dyad and Triad Census\nClass lecture 2d: Transitivity and Clustering Coefficient\n\nWeek 3 - Network Degree\nClass lecture 3a: Degree\nClass lecture 3b: Centrality vs. Centralization\nClass lecture 3c: Network Density\n\n\n\n > Academics > PS 753 > Glossary\n\n\nA social network is a set of actors (or points, or nodes, or agents) that may have relationships (or edges, or ties) with one another. Networks can have few or many actors, and one or more kinds of relations between pairs of actors.\n– (Hanneman, Robert A. and Riddle, Mark 2005, chap. 2)\n\nWeek 1 - Introduction\nClass lecture 1a: Nodes, Edges and Network Samples\n(Note: some terms overlap somewhat or have context-dependent synonyms.)\nnode / vertex\na junction in a network where two or more lines (edges) intersect; a dot connecting lines.\n\ntie / edge / link / relation\nlines connecting nodes, which indicate some sort of connection or relationship.\n\nnode / ego / actor\n\\(i\\) - the node being discussed or focused on; also note “Actors are described by their relations, not by their attributes.”1\n\nalter\n\\(j\\) - the node that \\(i\\) connects to\n\nnetwork population\n\\(n\\) - the size of the population or count of nodes\n\nWith set notation, we define \\(i\\) as a set of \\(n\\) elements: \\[i \\in 1, 2, 3 \\dots n\\]\nand \\(j\\) similarly as a set of \\(n\\) elements, except that \\(j\\) cannot equal \\(i\\) (a node cannot connect to itself): \\[j \\in 1, 2, 3 \\dots n, i \\ne j\\]\ninteraction threshold\na measure to determine whether two entities have a sufficient connection to be considered to have a link between them\n\nsnowball sample\nan entity group formed by starting with “a focal actor or set of actors”2 and “rolling outwards” to its connections until all nodes or actors (in a limited set) are located, or a decision to stop is made\n\negocentric name generator\na mode of building an entity group (an egocentric network) defined by connection to a single central node; like a snowball sample that doesn’t expand past the first set of connections\n\nClass lecture 1b: Ties and Adjacency Matrices\nadjacency matrix\nan \\(n \\times n\\) matrix depicting connections between \\(n\\) nodes as 0 or 1, where 1 is a connection (or vertex/edge) and 0 is the absence of one:\n\n\nA\nB\nC\nD\nA\n-\n0\n1\n0\nB\n0\n-\n1\n0\nC\n1\n1\n-\n0\nD\n0\n0\n0\n-\ndirected tie\na relationship where \\(W_{i,j} \\ne W_{j,i}\\), as might be used to represent a transfer of resources from one node to another; graphed with arrows\n\nsymmetric tie\na relationship where \\(W_{i,j} = W_{j,i}\\), with no direction; graphed with lines\n\nbinary tie\na tie where \\(W_{i,j}\\) is 0 or 1, as in the example above, indicating the absence or presence of a tie (also dichotomous)\n\nvalued tie\na tie where \\(W_{i,j}\\) is a value \\(v\\), indicating a weight or magnitude of the connection; may be graphed with line attributes such as weight, color, etc\n\nTies are symmetric or directed, and binary or valued.\nClass lecture 1c: Edgelists\nedgelist\na table indicating edges in a network, with at least “from” and “to” columns, and possibly additional columns for attributes or values\n\nFrom\nTo\nValue\nA\nB\n5\nA\nE\n2\nB\nA\n-1\nB\nC\n4\nB\nD\n2\nC\nA\n-4\nEdgelists are more efficient in sparse networks, as they only list actual connections rather than being a matrix of all possible connections\nHanneman, Robert A. and Riddle, Mark (2005), chapter 1 (Social network data):\nbinary measures of relations\nundirected relations, 0 or 1 for the absence or presence of a connection\n\nmultiple-category nominal measures of relations\ndirected relations with categories (like “friend, lover, business relationship, kin, or no relationship”)\n\ngrouped ordinal measures of relations\nmeasures that reflect a level of intensity or degree; often turned into binary measures by means of a threshold or cut-off\n\nfull-rank ordinal measures of relations\nan ordering from 1 to \\(n\\) of an actor’s relations (uncommon in social networks)\n\ninterval measures of relations\ncontinuous measures that express the strength of connections by comparison with others, to be able to say “this tie is twice as strong as that tie”; the “most advanced” level of measurement\n\nLazer (2011)\nhomophily\nthe idea that individuals who are similar to one another are more likely to form ties\n\nwhole network data\nrelational information about a whole set (or subset) of data, with all of the actors’ relations to each other considered\n\negocentric data\nrelational information about a set of nodes connected to one particular node and not to each other\n\ndiameter\nthe maximum degree of separation between any two nodes in the network\n\none-mode data\nties among one set (or category) of agent, such as nations in the context of trade\n\ntwo-mode data / bipartite data\nties between different sets (or categories) of agents, such as ties between nations and international organizations\n\nBorgatti et al. (2009)\nsociometry\n“a technique for eliciting and graphically representing individuals’ subjective feelings toward one another”\n\nstrength of weak ties (SWT) theory\nthe theory that one is likelier to hear new information from people they aren’t closely connected to in a network (c.f. homiphily)\n\ncentrality\na family of positional properties of nodes in a network\n\nFreeman’s betweenness\na type of centrality where a node is frequently along the shortest path between pairs of nodes, giving control over flow or power\n\nopportunity-based antecedents\n“the likelihood that two nodes will come into contact” - when considering the formation of ties\n\nbenefit-based antecedents\n“some kind of utility maximization or discomfort minimization that leads to tie formation”\n\nnode homogeneity\na category of node outcomes referring to the similarity of nodes\n\nperformance\na category of node outcomes referring to some good (such as strong performance)\n\nWeek 2 - Network Structure\nClass lecture 2a: Network Structure - Walks, Paths, and Distance\nConnections between nodes:\nadjacent\na direct connection between nodes; not necessarily bilateral in directed networks. A leading to B (“A adj B”) does not mean that B leads to A (\"B adj A)\n\nreachable\nwhether a node is reachable from another node, regardless of distance\n\ndistance\nthe number of ties (steps, edges) that must be traversed to reach a target node\n\nwalk\nsequence (not path, see below) that connects two nodes, consisting of the nodes and edges\n\ntrail\na walk that can only go through each edge / tie once, but can hit the same node more than once\n\npath\na trail that only hits each node and edge once; the start and end node may be the same\n\ngeodesic distance\nthe shortest path between two nodes; by definition, not a trail or walk because repeated segments wouldn’t be the shortest; with binary data, the number of edges between the nodes; with weighted data, might be a sum or some other calculation of “effort”\n\nClass lecture 2b: Graph Substructures and Components\nNetwork substructures\ndyad, triad, clique\ntwo, three, or four-or-more connected nodes\n\ncomplete graph\na network where every node is directly connected to every other node\n\nconnected graph\na network where every node is indirectly connected to every other node\n\nunconnected graph\na network where at least one node is unreachable\n\ncomponent\nthe set of all points that constitutes a connected subgraph within a network\n\nmain component\nthe largest component within a network\n\nminor component\na smaller one, possibly one of many\n\npendant\na node with only one link or edge to a network, “dangling”\n\nisolate\nan unconnected node\n\nClass lecture 2c: Dyad and Triad Census\nmutual dyad\na dyad where both nodes connect to each other, as in an undirected network\n\nasymmetric dyad\na dyad (in a directed network) where one node connects to another, but non-reciprocally (one way only)\n\nnull\na dyad of two unconnected nodes\n\n(empty / one edge / two path / triangle) triad\na triad with zero, one, two or three edges between three nodes (all four possible permutations)\n\nbalance theory\nthe theory that two nodes connected to a common node will also develop connections to each other\n\nglobal transitivity index\nthe proportion of triads in a network that are complete (with 3 connections between them)\n\nThere is a vocabulary for triads in directed networks describing the 16 possible permutations of ties (or the absence thereof) among 3 nodes, counting the number of mutual, asymmetric and null dyads, with direction indicators, like 003 or 120D - see the slide at 3:45\nvacuously transitive triad\na triad where (to be continued…) (5 of 16 possibilities)\n\nintransitive triad\na triad where (to be continued…) (7 of 16 possibilities)\n\ntransitive triad\na triad where (to be continued…) (4 of 16 possibilities)\n\n(See Alhazmi, Gokhale, and Doran (2015))\nClass lecture 2d: Transitivity and Clustering Coefficient\nlocal transitivity / local clustering coefficient\nthe likelihood that the neighbors of a node are also connected to each other; the number of connections that do exist over the number of connections that could exist\n\n In the example above, there are 8 nodes that Homer (center) could connect to; among those 8 nodes, there are \\(7 + 6 + 5 + 4 + 3 + 2 + 1 = 28\\) possible undirected connections (not connecting to Homer), and 9 of those 28 do exist, for a local clustering coefficient of \\(9/28 \\approx 0.32\\).\nWeek 3 - Network Degree\nClass lecture 3a: Degree\ndegree\nthe number of links that a node has; the number of nodes it’s connected to\n\ndegree distribution\na distribution showing the number of nodes of a network that have each degree level\n\nindegree\nthe number of links that a node receives in a directed network\n\noutdegree\nthe number of links that a node sends in a directed network\n\nClass lecture 3b: Centrality vs. Centralization\ncentrality\na measure of the prominence of one node relative to others; can be variously defined\n\n(degree) centrality\nproportional the the number of other nodes to which a node is linked\n\ncentralization\na property of a graph, referring to its overall cohesion; comparing most central point to all other points; ratio of the actual sum of differences to the maximum possible sum of differences\n\nClass lecture 3c: Network Density\nnetwork density\nnumber of ties as a proportion of the maximum possible number of ties; varies from 0 to 1, calculation will vary by whether network is undirected or directed (twice as many potential connections)\n\n\n\n\nAlhazmi, Huda, Swapna S. Gokhale, and Derek Doran. 2015. “Understanding Social Effects in Online Networks.” In 2015 International Conference on Computing, Networking and Communications (ICNC), 863–68. Garden Grove, CA, USA: IEEE. https://doi.org/10.1109/ICCNC.2015.7069459.\n\n\nBorgatti, Stephen P., Ajay Mehra, Daniel J. Brass, and Giuseppe Labianca. 2009. “Network Analysis in the Social Sciences.” Science, February. https://doi.org/10.1126/science.1165821.\n\n\nHanneman, Robert A., and Riddle, Mark. 2005. “Introduction to Social Network Methods.” Introduction to Social Network Methods. http://faculty.ucr.edu/~hanneman/nettext/.\n\n\nLazer, David. 2011. “Networks in Political Science: Back to the Future.” PS: Political Science & Politics 44 (1): 61–68. https://doi.org/10.1017/S1049096510001873.\n\n\nHanneman, Robert A. and Riddle, Mark (2005)↩︎\nHanneman, Robert A. and Riddle, Mark (2005)↩︎\n",
    "preview": {},
    "last_modified": "2022-02-08T09:07:49-05:00",
    "input_file": {}
  }
]
